{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Completo - AnÃ¡lisis y DetecciÃ³n de AnomalÃ­as en Planta Solar\n",
    "# Ejecutar las celdas en orden\n",
    "\n",
    "# ========================================\n",
    "# CELDA 1: ImportaciÃ³n de librerÃ­as\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from scipy.stats import chi2\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ConfiguraciÃ³n de visualizaciÃ³n\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 2: Carga de datos\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_solar_data():\n",
    "    \"\"\"Carga los tres archivos de datos solares\"\"\"\n",
    "    try:\n",
    "        # Cargar datos ambientales\n",
    "        env_data = pd.read_csv('environment_data.csv', parse_dates=['measured_on'])\n",
    "        print(f\"âœ“ Datos ambientales: {env_data.shape}\")\n",
    "        \n",
    "        # Cargar datos de irradiancia\n",
    "        irr_data = pd.read_csv('irradiance_data.csv', parse_dates=['measured_on'])\n",
    "        print(f\"âœ“ Datos de irradiancia: {irr_data.shape}\")\n",
    "        \n",
    "        # Cargar datos elÃ©ctricos\n",
    "        elec_data = pd.read_csv('chunk_electrical_data.csv', parse_dates=['measured_on'])\n",
    "        print(f\"âœ“ Datos elÃ©ctricos: {elec_data.shape}\")\n",
    "        \n",
    "        return env_data, irr_data, elec_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando datos: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "env_data, irr_data, elec_data = load_solar_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 3: ExploraciÃ³n de datos\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_data(df, name):\n",
    "    \"\"\"ExploraciÃ³n bÃ¡sica de cada dataset\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ANÃLISIS DE: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Info bÃ¡sica\n",
    "    print(f\"\\nğŸ“Š Forma: {df.shape}\")\n",
    "    print(f\"ğŸ“… Rango temporal: {df['measured_on'].min()} a {df['measured_on'].max()}\")\n",
    "    \n",
    "    # Valores faltantes\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.any():\n",
    "        print(f\"\\nâš ï¸ Valores faltantes:\")\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(f\"\\nâœ“ Sin valores faltantes\")\n",
    "    \n",
    "    # EstadÃ­sticas descriptivas (excluyendo fecha)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"\\nğŸ“ˆ EstadÃ­sticas descriptivas:\")\n",
    "    print(df[numeric_cols].describe().round(2))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Explorar cada dataset\n",
    "env_data = explore_data(env_data, \"DATOS AMBIENTALES\")\n",
    "irr_data = explore_data(irr_data, \"DATOS DE IRRADIANCIA\")\n",
    "elec_data = explore_data(elec_data, \"DATOS ELÃ‰CTRICOS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ========================================\n",
    "# CELDA 4: FusiÃ³n y limpieza de datos\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_merge_data(env_data, irr_data, elec_data):\n",
    "    \"\"\"\n",
    "    Limpia y fusiona los datasets por fecha\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ”§ PROCESAMIENTO DE DATOS...\")\n",
    "    \n",
    "    # Eliminar duplicados por fecha si existen\n",
    "    env_data = env_data.drop_duplicates(subset=['measured_on'])\n",
    "    irr_data = irr_data.drop_duplicates(subset=['measured_on'])\n",
    "    elec_data = elec_data.drop_duplicates(subset=['measured_on'])\n",
    "    \n",
    "    # Merge secuencial\n",
    "    print(\"\\nğŸ“Š Fusionando datasets...\")\n",
    "    \n",
    "    # Primero: ambiental con irradiancia\n",
    "    merged_data = pd.merge(env_data, irr_data, on='measured_on', how='inner')\n",
    "    print(f\"âœ“ Ambiente + Irradiancia: {merged_data.shape}\")\n",
    "    \n",
    "    # Segundo: resultado con elÃ©ctrico\n",
    "    merged_data = pd.merge(merged_data, elec_data, on='measured_on', how='inner')\n",
    "    print(f\"âœ“ Dataset final: {merged_data.shape}\")\n",
    "    \n",
    "    # Ordenar por fecha\n",
    "    merged_data = merged_data.sort_values('measured_on')\n",
    "    \n",
    "    # Manejar valores infinitos\n",
    "    merged_data = merged_data.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Imputar NaN con interpolaciÃ³n\n",
    "    numeric_cols = merged_data.select_dtypes(include=[np.number]).columns\n",
    "    merged_data[numeric_cols] = merged_data[numeric_cols].interpolate(method='linear', limit=5)\n",
    "    \n",
    "    # Eliminar filas con muchos NaN\n",
    "    merged_data = merged_data.dropna(thresh=len(merged_data.columns)*0.8)\n",
    "    \n",
    "    print(f\"\\nâœ… Dataset limpio final: {merged_data.shape}\")\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "# Fusionar y limpiar\n",
    "merged_data = clean_and_merge_data(env_data, irr_data, elec_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 5: IngenierÃ­a de caracterÃ­sticas\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Crea caracterÃ­sticas derivadas relevantes para anÃ¡lisis solar\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ”¬ CREANDO CARACTERÃSTICAS DERIVADAS...\")\n",
    "    \n",
    "    original_cols = len(df.columns)\n",
    "    \n",
    "    # CaracterÃ­sticas temporales\n",
    "    df['hour'] = df['measured_on'].dt.hour\n",
    "    df['day_of_year'] = df['measured_on'].dt.dayofyear\n",
    "    df['month'] = df['measured_on'].dt.month\n",
    "    \n",
    "    # Indicador dÃ­a/noche (aproximado)\n",
    "    df['is_daytime'] = ((df['hour'] >= 6) & (df['hour'] <= 18)).astype(int)\n",
    "    \n",
    "    # Potencia DC total de todos los inversores\n",
    "    dc_power_cols = []\n",
    "    for i in range(1, 25):\n",
    "        col_name = f'dc_power_inv_{i:02d}'\n",
    "        dc_current = f'inv_{i:02d}_dc_current_inv' \n",
    "        dc_voltage = f'inv_{i:02d}_dc_voltage_inv'\n",
    "        \n",
    "        # Buscar columnas que coincidan\n",
    "        current_cols = [c for c in df.columns if dc_current in c]\n",
    "        voltage_cols = [c for c in df.columns if dc_voltage in c]\n",
    "        \n",
    "        if current_cols and voltage_cols:\n",
    "            df[col_name] = df[current_cols[0]] * df[voltage_cols[0]]\n",
    "            dc_power_cols.append(col_name)\n",
    "    \n",
    "    # Potencia total DC y AC\n",
    "    ac_power_cols = [col for col in df.columns if 'ac_power' in col]\n",
    "    if dc_power_cols:\n",
    "        df['total_dc_power'] = df[dc_power_cols].sum(axis=1)\n",
    "    else:\n",
    "        df['total_dc_power'] = 0\n",
    "        \n",
    "    df['total_ac_power'] = df[ac_power_cols].sum(axis=1)\n",
    "    \n",
    "    # Eficiencia del sistema\n",
    "    df['system_efficiency'] = np.where(\n",
    "        df['total_dc_power'] > 0,\n",
    "        (df['total_ac_power'] / df['total_dc_power']) * 100,\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Performance Ratio (PR) - rendimiento respecto a condiciones ideales\n",
    "    df['performance_ratio'] = np.where(\n",
    "        df['poa_irradiance_o_149574'] > 50,  # Solo cuando hay sol significativo\n",
    "        df['total_ac_power'] / df['poa_irradiance_o_149574'],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # RelaciÃ³n temperatura-eficiencia (pÃ©rdidas tÃ©rmicas)\n",
    "    df['temp_impact'] = df['ambient_temperature_o_149575'] * df['system_efficiency']\n",
    "    \n",
    "    # Indicadores de anomalÃ­a por inversor\n",
    "    for i in range(1, 25):\n",
    "        ac_col = [c for c in ac_power_cols if f'inv_{i:02d}' in c]\n",
    "        if ac_col and len(ac_power_cols) > 0:\n",
    "            # DesviaciÃ³n del inversor respecto a la media\n",
    "            df[f'inv_{i:02d}_deviation'] = np.where(\n",
    "                df[ac_power_cols].std(axis=1) > 0,\n",
    "                (df[ac_col[0]] - df[ac_power_cols].mean(axis=1)) / df[ac_power_cols].std(axis=1),\n",
    "                0\n",
    "            )\n",
    "    \n",
    "    print(f\"âœ… CaracterÃ­sticas creadas: {len(df.columns) - original_cols} nuevas\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Crear caracterÃ­sticas\n",
    "featured_data = create_features(merged_data.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 6: SelecciÃ³n de variables relevantes\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_features(df):\n",
    "    \"\"\"\n",
    "    Selecciona las variables mÃ¡s relevantes para detecciÃ³n de anomalÃ­as\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ¯ SELECCIÃ“N DE CARACTERÃSTICAS RELEVANTES\")\n",
    "    \n",
    "    # Variables clave para anÃ¡lisis de anomalÃ­as en plantas solares\n",
    "    key_features = [\n",
    "        # Ambientales\n",
    "        'ambient_temperature_o_149575',\n",
    "        'wind_speed_o_149576',\n",
    "        'poa_irradiance_o_149574',\n",
    "        \n",
    "        # Rendimiento global\n",
    "        'total_dc_power',\n",
    "        'total_ac_power',\n",
    "        'system_efficiency',\n",
    "        'performance_ratio',\n",
    "        \n",
    "        # Temporales\n",
    "        'hour',\n",
    "        'month',\n",
    "        'is_daytime'\n",
    "    ]\n",
    "    \n",
    "    # AÃ±adir desviaciones de inversores\n",
    "    deviation_cols = [col for col in df.columns if 'deviation' in col]\n",
    "    key_features.extend(deviation_cols[:5])  # Top 5 inversores con mÃ¡s variaciÃ³n\n",
    "    \n",
    "    # Verificar que existen\n",
    "    key_features = [f for f in key_features if f in df.columns]\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Variables seleccionadas ({len(key_features)}):\")\n",
    "    for i, feat in enumerate(key_features, 1):\n",
    "        print(f\"   {i}. {feat}\")\n",
    "    \n",
    "    # Calcular correlaciones\n",
    "    corr_matrix = df[key_features].corr()\n",
    "    \n",
    "    # Visualizar matriz de correlaciÃ³n\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "                cmap='coolwarm', center=0, square=True, \n",
    "                linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Matriz de CorrelaciÃ³n - Variables Clave', fontsize=16, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # AnÃ¡lisis de correlaciones importantes\n",
    "    print(\"\\nğŸ” CORRELACIONES IMPORTANTES (|r| > 0.7):\")\n",
    "    high_corr = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "                high_corr.append({\n",
    "                    'Variable 1': corr_matrix.columns[i],\n",
    "                    'Variable 2': corr_matrix.columns[j],\n",
    "                    'CorrelaciÃ³n': corr_matrix.iloc[i, j]\n",
    "                })\n",
    "    \n",
    "    if high_corr:\n",
    "        corr_df = pd.DataFrame(high_corr).sort_values('CorrelaciÃ³n', \n",
    "                                                      key=abs, \n",
    "                                                      ascending=False)\n",
    "        print(corr_df.to_string(index=False))\n",
    "    \n",
    "    return key_features, corr_matrix\n",
    "\n",
    "# Seleccionar caracterÃ­sticas\n",
    "selected_features, correlation_matrix = select_relevant_features(featured_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ========================================\n",
    "# CELDA 7: PreparaciÃ³n de datos horarios\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_hourly_data(df, features):\n",
    "    \"\"\"\n",
    "    Prepara datos con promedios horarios para detecciÃ³n de anomalÃ­as\n",
    "    \"\"\"\n",
    "    print(\"\\nâ° PREPARANDO PROMEDIOS HORARIOS...\")\n",
    "    \n",
    "    # Crear columna de hora redondeada\n",
    "    df['hour_rounded'] = df['measured_on'].dt.floor('H')\n",
    "    \n",
    "    # Agrupar por hora y calcular promedios\n",
    "    # Incluir hour_rounded en el groupby\n",
    "    agg_dict = {feat: 'mean' for feat in features}\n",
    "    hourly_data = df.groupby('hour_rounded').agg(agg_dict).reset_index()\n",
    "    \n",
    "    # Recalcular la columna hour si no estÃ¡\n",
    "    if 'hour' not in hourly_data.columns:\n",
    "        hourly_data['hour'] = hourly_data['hour_rounded'].dt.hour\n",
    "    \n",
    "    # Filtrar solo horas diurnas para anÃ¡lisis solar\n",
    "    hourly_data_day = hourly_data[\n",
    "        (hourly_data['hour'] >= 6) & \n",
    "        (hourly_data['hour'] <= 18) &\n",
    "        (hourly_data['poa_irradiance_o_149574'] > 50)  # Con irradiancia significativa\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"âœ… Datos horarios preparados: {hourly_data_day.shape}\")\n",
    "    \n",
    "    # Normalizar datos\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(hourly_data_day[features])\n",
    "    \n",
    "    return hourly_data_day, features_scaled, scaler\n",
    "\n",
    "# Preparar datos\n",
    "hourly_data, X_scaled, scaler = prepare_hourly_data(featured_data, selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 8: Detector de AnomalÃ­as - Distancia Euclidiana\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_anomaly_detector(X, data, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Detecta anomalÃ­as usando distancia euclidiana\n",
    "    Marca el 5% mÃ¡s lejano como anomalÃ­as\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ”´ DETECTOR 1: DISTANCIA EUCLIDIANA\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Calcular centroide\n",
    "    centroid = np.mean(X, axis=0)\n",
    "    \n",
    "    # Calcular distancias euclidianas\n",
    "    distances = np.sqrt(np.sum((X - centroid)**2, axis=1))\n",
    "    \n",
    "    # Determinar threshold (percentil 95)\n",
    "    threshold = np.percentile(distances, (1 - contamination) * 100)\n",
    "    \n",
    "    # Identificar anomalÃ­as\n",
    "    anomalies = distances > threshold\n",
    "    \n",
    "    # AÃ±adir resultados al dataframe\n",
    "    data['euclidean_distance'] = distances\n",
    "    data['euclidean_anomaly'] = anomalies\n",
    "    \n",
    "    # EstadÃ­sticas\n",
    "    print(f\"ğŸ“Š EstadÃ­sticas:\")\n",
    "    print(f\"   - Distancia promedio: {np.mean(distances):.2f}\")\n",
    "    print(f\"   - Distancia mÃ¡xima: {np.max(distances):.2f}\")\n",
    "    print(f\"   - Threshold (95%): {threshold:.2f}\")\n",
    "    print(f\"   - AnomalÃ­as detectadas: {np.sum(anomalies)} ({np.sum(anomalies)/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    # VisualizaciÃ³n\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histograma de distancias\n",
    "    ax1.hist(distances, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "    ax1.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold = {threshold:.2f}')\n",
    "    ax1.set_xlabel('Distancia Euclidiana')\n",
    "    ax1.set_ylabel('Frecuencia')\n",
    "    ax1.set_title('DistribuciÃ³n de Distancias Euclidianas')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Serie temporal con anomalÃ­as\n",
    "    ax2.scatter(data['hour_rounded'], data['total_ac_power'], \n",
    "                c=data['euclidean_anomaly'], cmap='coolwarm', \n",
    "                alpha=0.6, s=50)\n",
    "    ax2.set_xlabel('Fecha')\n",
    "    ax2.set_ylabel('Potencia AC Total (W)')\n",
    "    ax2.set_title('AnomalÃ­as Detectadas - Distancia Euclidiana')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Fechas con anomalÃ­as\n",
    "    anomaly_dates = data[data['euclidean_anomaly']]['hour_rounded'].tolist()\n",
    "    \n",
    "    print(f\"\\nğŸ“… Primeras 10 fechas con anomalÃ­as:\")\n",
    "    for i, date in enumerate(anomaly_dates[:10], 1):\n",
    "        print(f\"   {i}. {date}\")\n",
    "    \n",
    "    return data, anomaly_dates\n",
    "\n",
    "# Aplicar detector euclidiano\n",
    "hourly_data, euclidean_anomalies = euclidean_anomaly_detector(\n",
    "    X_scaled, hourly_data.copy(), contamination=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 9: Detector de AnomalÃ­as - Distancia de Mahalanobis\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_anomaly_detector(X, data, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Detecta anomalÃ­as usando distancia de Mahalanobis\n",
    "    Considera las correlaciones entre variables\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸŸ¡ DETECTOR 2: DISTANCIA DE MAHALANOBIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Calcular media y covarianza\n",
    "    mean = np.mean(X, axis=0)\n",
    "    \n",
    "    # Usar Minimum Covariance Determinant para robustez\n",
    "    cov_estimator = EmpiricalCovariance()\n",
    "    cov_estimator.fit(X)\n",
    "    \n",
    "    # Calcular distancias de Mahalanobis\n",
    "    mahal_distances = cov_estimator.mahalanobis(X)\n",
    "    \n",
    "    # Determinar threshold usando distribuciÃ³n chi-cuadrado\n",
    "    p = X.shape[1]\n",
    "    threshold = chi2.ppf(1 - contamination, df=p)\n",
    "    \n",
    "    # Identificar anomalÃ­as\n",
    "    anomalies = mahal_distances > threshold\n",
    "    \n",
    "    # AÃ±adir resultados\n",
    "    data['mahalanobis_distance'] = mahal_distances\n",
    "    data['mahalanobis_anomaly'] = anomalies\n",
    "    \n",
    "    # EstadÃ­sticas\n",
    "    print(f\"ğŸ“Š EstadÃ­sticas:\")\n",
    "    print(f\"   - Dimensiones: {p} variables\")\n",
    "    print(f\"   - Distancia promedio: {np.mean(mahal_distances):.2f}\")\n",
    "    print(f\"   - Distancia mÃ¡xima: {np.max(mahal_distances):.2f}\")\n",
    "    print(f\"   - Threshold Ï‡Â²(95%): {threshold:.2f}\")\n",
    "    print(f\"   - AnomalÃ­as detectadas: {np.sum(anomalies)} ({np.sum(anomalies)/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    # VisualizaciÃ³n\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Q-Q plot para verificar distribuciÃ³n chi-cuadrado\n",
    "    stats.probplot(mahal_distances, dist=stats.chi2, sparams=(p,), plot=ax1)\n",
    "    ax1.set_title('Q-Q Plot: Mahalanobis vs Chi-cuadrado')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Scatter plot: Euclidiana vs Mahalanobis\n",
    "    ax2.scatter(data['euclidean_distance'], data['mahalanobis_distance'], \n",
    "                c=data['mahalanobis_anomaly'], cmap='viridis', \n",
    "                alpha=0.6, s=50)\n",
    "    ax2.set_xlabel('Distancia Euclidiana')\n",
    "    ax2.set_ylabel('Distancia de Mahalanobis')\n",
    "    ax2.set_title('ComparaciÃ³n de Distancias')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # AnÃ¡lisis de diferencias con Euclidiana\n",
    "    both_anomalies = data['euclidean_anomaly'] & data['mahalanobis_anomaly']\n",
    "    only_euclidean = data['euclidean_anomaly'] & ~data['mahalanobis_anomaly']\n",
    "    only_mahalanobis = ~data['euclidean_anomaly'] & data['mahalanobis_anomaly']\n",
    "    \n",
    "    print(f\"\\nğŸ” ComparaciÃ³n con Euclidiana:\")\n",
    "    print(f\"   - AnomalÃ­as en ambos: {np.sum(both_anomalies)}\")\n",
    "    print(f\"   - Solo Euclidiana: {np.sum(only_euclidean)}\")\n",
    "    print(f\"   - Solo Mahalanobis: {np.sum(only_mahalanobis)}\")\n",
    "    \n",
    "    # Fechas con anomalÃ­as\n",
    "    anomaly_dates = data[data['mahalanobis_anomaly']]['hour_rounded'].tolist()\n",
    "    \n",
    "    print(f\"\\nğŸ“… Primeras 10 fechas con anomalÃ­as (Mahalanobis):\")\n",
    "    for i, date in enumerate(anomaly_dates[:10], 1):\n",
    "        print(f\"   {i}. {date}\")\n",
    "    \n",
    "    return data, anomaly_dates\n",
    "\n",
    "# Aplicar detector Mahalanobis\n",
    "hourly_data, mahalanobis_anomalies = mahalanobis_anomaly_detector(\n",
    "    X_scaled, hourly_data.copy(), contamination=0.05\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 10: Detector de AnomalÃ­as - Isolation Forest\n",
    "# ========================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolation_forest_detector(X, data, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Detecta anomalÃ­as usando Isolation Forest\n",
    "    Algoritmo basado en Ã¡rboles de decisiÃ³n\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸŸ¢ DETECTOR 3: ISOLATION FOREST\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Configurar y entrenar Isolation Forest\n",
    "    iso_forest = IsolationForest(\n",
    "        contamination=contamination,\n",
    "        random_state=42,\n",
    "        n_estimators=100,\n",
    "        max_samples='auto',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Predecir anomalÃ­as (-1 = anomalÃ­a, 1 = normal)\n",
    "    predictions = iso_forest.fit_predict(X)\n",
    "    anomaly_scores = iso_forest.score_samples(X)\n",
    "    \n",
    "    # Convertir a booleano (True = anomalÃ­a)\n",
    "    anomalies = predictions == -1\n",
    "    \n",
    "    # AÃ±adir resultados\n",
    "    data['isolation_score'] = anomaly_scores\n",
    "    data['isolation_anomaly'] = anomalies\n",
    "    \n",
    "    # EstadÃ­sticas\n",
    "    print(f\"ğŸ“Š EstadÃ­sticas:\")\n",
    "    print(f\"   - Score promedio: {np.mean(anomaly_scores):.3f}\")\n",
    "    print(f\"   - Score mÃ­nimo (mÃ¡s anÃ³malo): {np.min(anomaly_scores):.3f}\")\n",
    "    print(f\"   - AnomalÃ­as detectadas: {np.sum(anomalies)} ({np.sum(anomalies)/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    # VisualizaciÃ³n comparativa\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. DistribuciÃ³n de scores\n",
    "    axes[0, 0].hist(anomaly_scores, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('Anomaly Score')\n",
    "    axes[0, 0].set_ylabel('Frecuencia')\n",
    "    axes[0, 0].set_title('DistribuciÃ³n de Scores - Isolation Forest')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. ComparaciÃ³n con otros mÃ©todos\n",
    "    methods = ['Euclidiana', 'Mahalanobis', 'Isolation Forest']\n",
    "    anomaly_counts = [\n",
    "        np.sum(data['euclidean_anomaly']),\n",
    "        np.sum(data['mahalanobis_anomaly']),\n",
    "        np.sum(data['isolation_anomaly'])\n",
    "    ]\n",
    "    \n",
    "    axes[0, 1].bar(methods, anomaly_counts, color=['red', 'yellow', 'green'], alpha=0.7)\n",
    "    axes[0, 1].set_ylabel('NÃºmero de AnomalÃ­as')\n",
    "    axes[0, 1].set_title('ComparaciÃ³n de MÃ©todos')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3. Venn diagram conceptual - Overlap de mÃ©todos\n",
    "    all_methods = data['euclidean_anomaly'] & data['mahalanobis_anomaly'] & data['isolation_anomaly']\n",
    "    eucl_maha = data['euclidean_anomaly'] & data['mahalanobis_anomaly'] & ~data['isolation_anomaly']\n",
    "    eucl_iso = data['euclidean_anomaly'] & ~data['mahalanobis_anomaly'] & data['isolation_anomaly']\n",
    "    maha_iso = ~data['euclidean_anomaly'] & data['mahalanobis_anomaly'] & data['isolation_anomaly']\n",
    "    \n",
    "    overlap_data = {\n",
    "        'Todos': np.sum(all_methods),\n",
    "        'Eucl+Maha': np.sum(eucl_maha),\n",
    "        'Eucl+Iso': np.sum(eucl_iso),\n",
    "        'Maha+Iso': np.sum(maha_iso),\n",
    "        'Solo Eucl': np.sum(data['euclidean_anomaly'] & ~data['mahalanobis_anomaly'] & ~data['isolation_anomaly']),\n",
    "        'Solo Maha': np.sum(~data['euclidean_anomaly'] & data['mahalanobis_anomaly'] & ~data['isolation_anomaly']),\n",
    "        'Solo Iso': np.sum(~data['euclidean_anomaly'] & ~data['mahalanobis_anomaly'] & data['isolation_anomaly'])\n",
    "    }\n",
    "    \n",
    "    axes[1, 0].bar(overlap_data.keys(), overlap_data.values(), alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('CombinaciÃ³n de MÃ©todos')\n",
    "    axes[1, 0].set_ylabel('NÃºmero de AnomalÃ­as')\n",
    "    axes[1, 0].set_title('IntersecciÃ³n de Detecciones')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Serie temporal con todos los mÃ©todos\n",
    "    axes[1, 1].scatter(data['hour_rounded'], data['performance_ratio'],\n",
    "                      c='blue', alpha=0.3, s=20, label='Normal')\n",
    "    \n",
    "    # Marcar anomalÃ­as de cada mÃ©todo\n",
    "    for method, color, marker in [\n",
    "        ('euclidean_anomaly', 'red', 'o'),\n",
    "        ('mahalanobis_anomaly', 'yellow', 's'),\n",
    "        ('isolation_anomaly', 'green', '^')\n",
    "    ]:\n",
    "        anomaly_data = data[data[method]]\n",
    "        if len(anomaly_data) > 0:\n",
    "            axes[1, 1].scatter(anomaly_data['hour_rounded'], \n",
    "                              anomaly_data['performance_ratio'],\n",
    "                              c=color, s=50, marker=marker, \n",
    "                              label=method.replace('_anomaly', '').title(),\n",
    "                              edgecolors='black', linewidths=0.5)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Fecha')\n",
    "    axes[1, 1].set_ylabel('Performance Ratio')\n",
    "    axes[1, 1].set_title('AnomalÃ­as Detectadas por Cada MÃ©todo')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Fechas con anomalÃ­as\n",
    "    anomaly_dates = data[data['isolation_anomaly']]['hour_rounded'].tolist()\n",
    "    \n",
    "    print(f\"\\nğŸ“… Primeras 10 fechas con anomalÃ­as (Isolation Forest):\")\n",
    "    for i, date in enumerate(anomaly_dates[:10], 1):\n",
    "        print(f\"   {i}. {date}\")\n",
    "    \n",
    "    # AnÃ¡lisis de sensibilidad a correlaciones\n",
    "    only_mahalanobis = np.sum(~data['euclidean_anomaly'] & data['mahalanobis_anomaly'] & ~data['isolation_anomaly'])\n",
    "    print(f\"\\nğŸ” ANÃLISIS DE SENSIBILIDAD A CORRELACIONES:\")\n",
    "    print(f\"\\nMahalanobis detectÃ³ {only_mahalanobis} anomalÃ­as Ãºnicas\")\n",
    "    print(\"Esto sugiere que Mahalanobis es MÃS sensible a correlaciones porque:\")\n",
    "    print(\"- Considera la estructura de covarianza de los datos\")\n",
    "    print(\"- Detecta puntos que son anÃ³malos en el contexto multivariado\")\n",
    "    print(\"- Identifica observaciones que rompen patrones de correlaciÃ³n normales\")\n",
    "    \n",
    "    return data, anomaly_dates\n",
    "\n",
    "# Aplicar Isolation Forest\n",
    "hourly_data, isolation_anomalies = isolation_forest_detector(\n",
    "    X_scaled, hourly_data.copy(), contamination=0.05\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 11: AnÃ¡lisis integrado y reporte final\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anomaly_report(data):\n",
    "    \"\"\"\n",
    "    Genera reporte consolidado de anomalÃ­as detectadas\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ“Š REPORTE CONSOLIDADO DE ANOMALÃAS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Crear score de severidad combinado\n",
    "    data['anomaly_count'] = (\n",
    "        data['euclidean_anomaly'].astype(int) +\n",
    "        data['mahalanobis_anomaly'].astype(int) +\n",
    "        data['isolation_anomaly'].astype(int)\n",
    "    )\n",
    "    \n",
    "    # Normalizar scores para severidad (0-100)\n",
    "    data['euclidean_severity'] = (data['euclidean_distance'] / \n",
    "                                  data['euclidean_distance'].max() * 100)\n",
    "    data['mahalanobis_severity'] = (data['mahalanobis_distance'] / \n",
    "                                    data['mahalanobis_distance'].max() * 100)\n",
    "    data['isolation_severity'] = ((1 - (data['isolation_score'] - \n",
    "                                       data['isolation_score'].min()) / \n",
    "                                  (data['isolation_score'].max() - \n",
    "                                   data['isolation_score'].min())) * 100)\n",
    "    \n",
    "    # Score de severidad promedio ponderado\n",
    "    data['severity_score'] = (\n",
    "        0.3 * data['euclidean_severity'] +\n",
    "        0.4 * data['mahalanobis_severity'] +  # Mayor peso por sensibilidad a correlaciones\n",
    "        0.3 * data['isolation_severity']\n",
    "    )\n",
    "    \n",
    "    # Filtrar anomalÃ­as (al menos 1 mÃ©todo)\n",
    "    anomalies = data[data['anomaly_count'] > 0].copy()\n",
    "    anomalies = anomalies.sort_values('severity_score', ascending=False)\n",
    "    \n",
    "    # Top 20 anomalÃ­as mÃ¡s severas\n",
    "    print(f\"\\nğŸš¨ TOP 20 ANOMALÃAS MÃS SEVERAS:\")\n",
    "    print(f\"{'Fecha':<20} {'MÃ©todos':<10} {'Severidad':<10} {'PR':<8} {'Efic%':<8} {'Irrad':<8}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for idx, row in anomalies.head(20).iterrows():\n",
    "        methods = row['anomaly_count']\n",
    "        date = row['hour_rounded'].strftime('%Y-%m-%d %H:%M')\n",
    "        severity = row['severity_score']\n",
    "        pr = row['performance_ratio']\n",
    "        eff = row['system_efficiency']\n",
    "        irr = row['poa_irradiance_o_149574']\n",
    "        \n",
    "        print(f\"{date:<20} {methods:<10} {severity:>8.1f} {pr:>8.2f} {eff:>8.1f} {irr:>8.0f}\")\n",
    "    \n",
    "    # Guardar lista completa\n",
    "    anomaly_list = anomalies[['hour_rounded', 'anomaly_count', 'severity_score',\n",
    "                              'euclidean_anomaly', 'mahalanobis_anomaly', \n",
    "                              'isolation_anomaly', 'performance_ratio',\n",
    "                              'system_efficiency', 'total_ac_power']].copy()\n",
    "    \n",
    "    # AnÃ¡lisis por mÃ©todo\n",
    "    print(f\"\\nğŸ“ˆ RESUMEN POR MÃ‰TODO:\")\n",
    "    print(f\"   - Euclidiana: {np.sum(data['euclidean_anomaly'])} anomalÃ­as\")\n",
    "    print(f\"   - Mahalanobis: {np.sum(data['mahalanobis_anomaly'])} anomalÃ­as\")\n",
    "    print(f\"   - Isolation Forest: {np.sum(data['isolation_anomaly'])} anomalÃ­as\")\n",
    "    print(f\"   - Detectadas por 3 mÃ©todos: {np.sum(data['anomaly_count'] == 3)}\")\n",
    "    print(f\"   - Detectadas por 2 mÃ©todos: {np.sum(data['anomaly_count'] == 2)}\")\n",
    "    print(f\"   - Detectadas por 1 mÃ©todo: {np.sum(data['anomaly_count'] == 1)}\")\n",
    "    \n",
    "    # VisualizaciÃ³n final\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "    \n",
    "    # Timeline de severidad\n",
    "    scatter = ax1.scatter(data['hour_rounded'], data['severity_score'],\n",
    "                         c=data['anomaly_count'], cmap='YlOrRd', \n",
    "                         s=50, alpha=0.6)\n",
    "    ax1.set_xlabel('Fecha')\n",
    "    ax1.set_ylabel('Score de Severidad (0-100)')\n",
    "    ax1.set_title('Timeline de AnomalÃ­as - Severidad y Consenso entre MÃ©todos')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax1, label='NÃºmero de mÃ©todos')\n",
    "    \n",
    "    # DistribuciÃ³n de severidad por nÃºmero de mÃ©todos\n",
    "    for i in range(4):\n",
    "        subset = data[data['anomaly_count'] == i]['severity_score']\n",
    "        if len(subset) > 0:\n",
    "            ax2.hist(subset, bins=30, alpha=0.6, \n",
    "                    label=f'{i} mÃ©todos', density=True)\n",
    "    \n",
    "    ax2.set_xlabel('Score de Severidad')\n",
    "    ax2.set_ylabel('Densidad')\n",
    "    ax2.set_title('DistribuciÃ³n de Severidad por Consenso de MÃ©todos')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return anomaly_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 12: Respuestas finales y guardar resultados\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESPUESTAS A LAS PREGUNTAS DEL ANÃLISIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nâ“ Â¿QuÃ© mÃ©todo parece mÃ¡s sensible a las correlaciones?\")\n",
    "print(\"\\nâœ… RESPUESTA: La Distancia de Mahalanobis es el mÃ©todo MÃS SENSIBLE a las correlaciones\")\n",
    "print(\"\\nJustificaciÃ³n:\")\n",
    "print(\"1. Mahalanobis considera la matriz de covarianza completa de los datos\")\n",
    "print(\"2. Detecta anomalÃ­as en el contexto multivariado (relaciones entre variables)\")\n",
    "print(\"3. Euclidiana trata todas las dimensiones independientemente\")\n",
    "print(\"4. Isolation Forest se enfoca en aislamiento estructural, no correlaciones directas\")\n",
    "\n",
    "only_mahalanobis = np.sum(~hourly_data['euclidean_anomaly'] & \n",
    "                          hourly_data['mahalanobis_anomaly'] & \n",
    "                          ~hourly_data['isolation_anomaly'])\n",
    "\n",
    "print(f\"\\nEvidencia: Mahalanobis detectÃ³ {only_mahalanobis} anomalÃ­as Ãºnicas\")\n",
    "print(\"que los otros mÃ©todos no identificaron, sugiriendo sensibilidad a patrones\")\n",
    "print(\"de correlaciÃ³n anormales.\")\n",
    "\n",
    "# Guardar resultados\n",
    "print(\"\\nğŸ’¾ Guardando resultados...\")\n",
    "hourly_data.to_csv('anomalias_detectadas_planta_solar.csv', index=False)\n",
    "print(\"âœ… Archivo guardado: anomalias_detectadas_planta_solar.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 13: Modelo ensamblado (diseÃ±o conceptual)\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISEÃ‘O DE MODELO ENSAMBLADO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "ğŸ“‹ DIAGRAMA DE FLUJO - MODELO ENSAMBLADO DE DETECCIÃ“N DE ANOMALÃAS\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Datos de       â”‚\n",
    "â”‚  Entrada        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Preprocesamientoâ”‚\n",
    "â”‚ - Limpieza      â”‚\n",
    "â”‚ - NormalizaciÃ³n â”‚\n",
    "â”‚ - Features      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â–¼         â–¼         â–¼          â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚Euclideanâ”‚â”‚Mahalanobisâ”‚â”‚Isolationâ”‚  â”‚\n",
    "â”‚Distance â”‚â”‚Distance   â”‚â”‚Forest   â”‚  â”‚\n",
    "â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚\n",
    "     â”‚          â”‚           â”‚       â”‚\n",
    "     â–¼          â–¼           â–¼       â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚    CÃ¡lculo de Scores        â”‚    â”‚\n",
    "â”‚  - Score_E (0-1)           â”‚    â”‚\n",
    "â”‚  - Score_M (0-1)           â”‚    â”‚\n",
    "â”‚  - Score_I (0-1)           â”‚    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "               â”‚                    â”‚\n",
    "               â–¼                    â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  ENSAMBLADO PONDERADO       â”‚    â”‚\n",
    "â”‚  Score_final = w1*Score_E + â”‚    â”‚\n",
    "â”‚                w2*Score_M + â”‚    â”‚\n",
    "â”‚                w3*Score_I   â”‚    â”‚\n",
    "â”‚  (w1=0.25, w2=0.45, w3=0.30)â”‚    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "               â”‚                    â”‚\n",
    "               â–¼                    â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  MÃ‰TRICA DE SEVERIDAD       â”‚    â”‚\n",
    "â”‚  Severity = Score_final Ã— F â”‚    â”‚\n",
    "â”‚  F = factor contextual      â”‚    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "               â”‚                    â”‚\n",
    "               â–¼                    â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  CLASIFICACIÃ“N FINAL        â”‚    â”‚\n",
    "â”‚  - Normal (S < 30)         â”‚    â”‚\n",
    "â”‚  - Leve (30 â‰¤ S < 50)     â”‚    â”‚\n",
    "â”‚  - Moderada (50 â‰¤ S < 70) â”‚    â”‚\n",
    "â”‚  - Severa (S â‰¥ 70)        â”‚    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "                                   â”‚\n",
    "               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "               â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  RETROALIMENTACIÃ“N          â”‚\n",
    "â”‚  - ValidaciÃ³n experta      â”‚\n",
    "â”‚  - Ajuste de pesos         â”‚\n",
    "â”‚  - Reentrenamiento         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\")\n",
    "\n",
    "# Ejemplo de implementaciÃ³n\n",
    "def ensemble_anomaly_score(eucl_score, maha_score, iso_score, \n",
    "                          hour, irradiance, affected_inverters):\n",
    "    \"\"\"\n",
    "    Calcula score de anomalÃ­a ensamblado con factor contextual\n",
    "    \"\"\"\n",
    "    # Pesos base\n",
    "    w1, w2, w3 = 0.25, 0.45, 0.30\n",
    "    \n",
    "    # Score ponderado base\n",
    "    base_score = w1 * eucl_score + w2 * maha_score + w3 * iso_score\n",
    "    \n",
    "    # Factor contextual\n",
    "    night_factor = 2.0 if hour < 6 or hour > 20 else 1.0\n",
    "    low_irr_factor = 1.5 if irradiance < 100 else 1.0\n",
    "    multi_inv_factor = 1 + (affected_inverters / 24)\n",
    "    \n",
    "    # Score final\n",
    "    contextual_factor = night_factor * low_irr_factor * multi_inv_factor\n",
    "    final_score = min(100, base_score * contextual_factor)\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "print(\"\\nâœ… AnÃ¡lisis completado exitosamente!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
