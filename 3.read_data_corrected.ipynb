{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Completo - An√°lisis y Detecci√≥n de Anomal√≠as en Planta Solar\n",
    "# Ejecutar las celdas en orden\n",
    "\n",
    "# ========================================\n",
    "# CELDA 1: Importaci√≥n de librer√≠as\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from scipy.stats import chi2\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 2: Carga de datos\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_solar_data():\n",
    "    \"\"\"Carga los tres archivos de datos solares\"\"\"\n",
    "    try:\n",
    "        # Cargar datos ambientales\n",
    "        env_data = pd.read_csv('environment_data.csv', parse_dates=['measured_on'])\n",
    "        print(f\"‚úì Datos ambientales: {env_data.shape}\")\n",
    "        \n",
    "        # Cargar datos de irradiancia\n",
    "        irr_data = pd.read_csv('irradiance_data.csv', parse_dates=['measured_on'])\n",
    "        print(f\"‚úì Datos de irradiancia: {irr_data.shape}\")\n",
    "        \n",
    "        # Cargar datos el√©ctricos\n",
    "        elec_data = pd.read_csv('chunk_electrical_data.csv', parse_dates=['measured_on'])\n",
    "        print(f\"‚úì Datos el√©ctricos: {elec_data.shape}\")\n",
    "        \n",
    "        return env_data, irr_data, elec_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error cargando datos: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "env_data, irr_data, elec_data = load_solar_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 3: Exploraci√≥n de datos\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_data(df, name):\n",
    "    \"\"\"Exploraci√≥n b√°sica de cada dataset\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"AN√ÅLISIS DE: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Info b√°sica\n",
    "    print(f\"\\nüìä Forma: {df.shape}\")\n",
    "    print(f\"üìÖ Rango temporal: {df['measured_on'].min()} a {df['measured_on'].max()}\")\n",
    "    \n",
    "    # Valores faltantes\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.any():\n",
    "        print(f\"\\n‚ö†Ô∏è Valores faltantes:\")\n",
    "        print(missing[missing > 0])\n",
    "    else:\n",
    "        print(f\"\\n‚úì Sin valores faltantes\")\n",
    "    \n",
    "    # Estad√≠sticas descriptivas (excluyendo fecha)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"\\nüìà Estad√≠sticas descriptivas:\")\n",
    "    print(df[numeric_cols].describe().round(2))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Explorar cada dataset\n",
    "env_data = explore_data(env_data, \"DATOS AMBIENTALES\")\n",
    "irr_data = explore_data(irr_data, \"DATOS DE IRRADIANCIA\")\n",
    "elec_data = explore_data(elec_data, \"DATOS EL√âCTRICOS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ========================================\n",
    "# CELDA 4: Fusi√≥n y limpieza de datos\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_merge_data(env_data, irr_data, elec_data):\n",
    "    \"\"\"\n",
    "    Limpia y fusiona los datasets por fecha\n",
    "    \"\"\"\n",
    "    print(\"\\nüîß PROCESAMIENTO DE DATOS...\")\n",
    "    \n",
    "    # Eliminar duplicados por fecha si existen\n",
    "    env_data = env_data.drop_duplicates(subset=['measured_on'])\n",
    "    irr_data = irr_data.drop_duplicates(subset=['measured_on'])\n",
    "    elec_data = elec_data.drop_duplicates(subset=['measured_on'])\n",
    "    \n",
    "    # Merge secuencial\n",
    "    print(\"\\nüìä Fusionando datasets...\")\n",
    "    \n",
    "    # Primero: ambiental con irradiancia\n",
    "    merged_data = pd.merge(env_data, irr_data, on='measured_on', how='inner')\n",
    "    print(f\"‚úì Ambiente + Irradiancia: {merged_data.shape}\")\n",
    "    \n",
    "    # Segundo: resultado con el√©ctrico\n",
    "    merged_data = pd.merge(merged_data, elec_data, on='measured_on', how='inner')\n",
    "    print(f\"‚úì Dataset final: {merged_data.shape}\")\n",
    "    \n",
    "    # Ordenar por fecha\n",
    "    merged_data = merged_data.sort_values('measured_on')\n",
    "    \n",
    "    # Manejar valores infinitos\n",
    "    merged_data = merged_data.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Imputar NaN con interpolaci√≥n\n",
    "    numeric_cols = merged_data.select_dtypes(include=[np.number]).columns\n",
    "    merged_data[numeric_cols] = merged_data[numeric_cols].interpolate(method='linear', limit=5)\n",
    "    \n",
    "    # Eliminar filas con muchos NaN\n",
    "    merged_data = merged_data.dropna(thresh=len(merged_data.columns)*0.8)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Dataset limpio final: {merged_data.shape}\")\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "# Fusionar y limpiar\n",
    "merged_data = clean_and_merge_data(env_data, irr_data, elec_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 5: Ingenier√≠a de caracter√≠sticas\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Crea caracter√≠sticas derivadas relevantes para an√°lisis solar\n",
    "    \"\"\"\n",
    "    print(\"\\nüî¨ CREANDO CARACTER√çSTICAS DERIVADAS...\")\n",
    "    \n",
    "    original_cols = len(df.columns)\n",
    "    \n",
    "    # Caracter√≠sticas temporales\n",
    "    df['hour'] = df['measured_on'].dt.hour\n",
    "    df['day_of_year'] = df['measured_on'].dt.dayofyear\n",
    "    df['month'] = df['measured_on'].dt.month\n",
    "    \n",
    "    # Indicador d√≠a/noche (aproximado)\n",
    "    df['is_daytime'] = ((df['hour'] >= 6) & (df['hour'] <= 18)).astype(int)\n",
    "    \n",
    "    # Potencia DC total de todos los inversores\n",
    "    dc_power_cols = []\n",
    "    for i in range(1, 25):\n",
    "        col_name = f'dc_power_inv_{i:02d}'\n",
    "        dc_current = f'inv_{i:02d}_dc_current_inv' \n",
    "        dc_voltage = f'inv_{i:02d}_dc_voltage_inv'\n",
    "        \n",
    "        # Buscar columnas que coincidan\n",
    "        current_cols = [c for c in df.columns if dc_current in c]\n",
    "        voltage_cols = [c for c in df.columns if dc_voltage in c]\n",
    "        \n",
    "        if current_cols and voltage_cols:\n",
    "            df[col_name] = df[current_cols[0]] * df[voltage_cols[0]]\n",
    "            dc_power_cols.append(col_name)\n",
    "    \n",
    "    # Potencia total DC y AC\n",
    "    ac_power_cols = [col for col in df.columns if 'ac_power' in col]\n",
    "    if dc_power_cols:\n",
    "        df['total_dc_power'] = df[dc_power_cols].sum(axis=1)\n",
    "    else:\n",
    "        df['total_dc_power'] = 0\n",
    "        \n",
    "    df['total_ac_power'] = df[ac_power_cols].sum(axis=1)\n",
    "    \n",
    "    # Eficiencia del sistema\n",
    "    df['system_efficiency'] = np.where(\n",
    "        df['total_dc_power'] > 0,\n",
    "        (df['total_ac_power'] / df['total_dc_power']) * 100,\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Performance Ratio (PR) - rendimiento respecto a condiciones ideales\n",
    "    df['performance_ratio'] = np.where(\n",
    "        df['poa_irradiance_o_149574'] > 50,  # Solo cuando hay sol significativo\n",
    "        df['total_ac_power'] / df['poa_irradiance_o_149574'],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Relaci√≥n temperatura-eficiencia (p√©rdidas t√©rmicas)\n",
    "    df['temp_impact'] = df['ambient_temperature_o_149575'] * df['system_efficiency']\n",
    "    \n",
    "    # Indicadores de anomal√≠a por inversor\n",
    "    for i in range(1, 25):\n",
    "        ac_col = [c for c in ac_power_cols if f'inv_{i:02d}' in c]\n",
    "        if ac_col and len(ac_power_cols) > 0:\n",
    "            # Desviaci√≥n del inversor respecto a la media\n",
    "            df[f'inv_{i:02d}_deviation'] = np.where(\n",
    "                df[ac_power_cols].std(axis=1) > 0,\n",
    "                (df[ac_col[0]] - df[ac_power_cols].mean(axis=1)) / df[ac_power_cols].std(axis=1),\n",
    "                0\n",
    "            )\n",
    "    \n",
    "    print(f\"‚úÖ Caracter√≠sticas creadas: {len(df.columns) - original_cols} nuevas\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Crear caracter√≠sticas\n",
    "featured_data = create_features(merged_data.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 6: Selecci√≥n de variables relevantes\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_features(df):\n",
    "    \"\"\"\n",
    "    Selecciona las variables m√°s relevantes para detecci√≥n de anomal√≠as\n",
    "    \"\"\"\n",
    "    print(\"\\nüéØ SELECCI√ìN DE CARACTER√çSTICAS RELEVANTES\")\n",
    "    \n",
    "    # Variables clave para an√°lisis de anomal√≠as en plantas solares\n",
    "    key_features = [\n",
    "        # Ambientales\n",
    "        'ambient_temperature_o_149575',\n",
    "        'wind_speed_o_149576',\n",
    "        'poa_irradiance_o_149574',\n",
    "        \n",
    "        # Rendimiento global\n",
    "        'total_dc_power',\n",
    "        'total_ac_power',\n",
    "        'system_efficiency',\n",
    "        'performance_ratio',\n",
    "        \n",
    "        # Temporales\n",
    "        'hour',\n",
    "        'month',\n",
    "        'is_daytime'\n",
    "    ]\n",
    "    \n",
    "    # A√±adir desviaciones de inversores\n",
    "    deviation_cols = [col for col in df.columns if 'deviation' in col]\n",
    "    key_features.extend(deviation_cols[:5])  # Top 5 inversores con m√°s variaci√≥n\n",
    "    \n",
    "    # Verificar que existen\n",
    "    key_features = [f for f in key_features if f in df.columns]\n",
    "    \n",
    "    print(f\"\\nüìã Variables seleccionadas ({len(key_features)}):\")\n",
    "    for i, feat in enumerate(key_features, 1):\n",
    "        print(f\"   {i}. {feat}\")\n",
    "    \n",
    "    # Calcular correlaciones\n",
    "    corr_matrix = df[key_features].corr()\n",
    "    \n",
    "    # Visualizar matriz de correlaci√≥n\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "                cmap='coolwarm', center=0, square=True, \n",
    "                linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Matriz de Correlaci√≥n - Variables Clave', fontsize=16, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # An√°lisis de correlaciones importantes\n",
    "    print(\"\\nüîç CORRELACIONES IMPORTANTES (|r| > 0.7):\")\n",
    "    high_corr = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > 0.7:\n",
    "                high_corr.append({\n",
    "                    'Variable 1': corr_matrix.columns[i],\n",
    "                    'Variable 2': corr_matrix.columns[j],\n",
    "                    'Correlaci√≥n': corr_matrix.iloc[i, j]\n",
    "                })\n",
    "    \n",
    "    if high_corr:\n",
    "        corr_df = pd.DataFrame(high_corr).sort_values('Correlaci√≥n', \n",
    "                                                      key=abs, \n",
    "                                                      ascending=False)\n",
    "        print(corr_df.to_string(index=False))\n",
    "    \n",
    "    return key_features, corr_matrix\n",
    "\n",
    "# Seleccionar caracter√≠sticas\n",
    "selected_features, correlation_matrix = select_relevant_features(featured_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ========================================\n",
    "# CELDA 7: Preparaci√≥n de datos horarios\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_hourly_data(df, features):\n",
    "    \"\"\"\n",
    "    Prepara datos con promedios horarios para detecci√≥n de anomal√≠as\n",
    "    \"\"\"\n",
    "    print(\"\\n‚è∞ PREPARANDO PROMEDIOS HORARIOS...\")\n",
    "    \n",
    "    # Crear columna de hora redondeada\n",
    "    df['hour_rounded'] = df['measured_on'].dt.floor('H')\n",
    "    \n",
    "    # Agrupar por hora y calcular promedios\n",
    "    # Incluir hour_rounded en el groupby\n",
    "    agg_dict = {feat: 'mean' for feat in features}\n",
    "    hourly_data = df.groupby('hour_rounded').agg(agg_dict).reset_index()\n",
    "    \n",
    "    # Recalcular la columna hour si no est√°\n",
    "    if 'hour' not in hourly_data.columns:\n",
    "        hourly_data['hour'] = hourly_data['hour_rounded'].dt.hour\n",
    "    \n",
    "    # Filtrar solo horas diurnas para an√°lisis solar\n",
    "    hourly_data_day = hourly_data[\n",
    "        (hourly_data['hour'] >= 6) & \n",
    "        (hourly_data['hour'] <= 18) &\n",
    "        (hourly_data['poa_irradiance_o_149574'] > 50)  # Con irradiancia significativa\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"‚úÖ Datos horarios preparados: {hourly_data_day.shape}\")\n",
    "    \n",
    "    # Normalizar datos\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(hourly_data_day[features])\n",
    "    \n",
    "    return hourly_data_day, features_scaled, scaler\n",
    "\n",
    "# Preparar datos\n",
    "hourly_data, X_scaled, scaler = prepare_hourly_data(featured_data, selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 8: Detector de Anomal√≠as - Distancia Euclidiana\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_anomaly_detector(X, data, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Detecta anomal√≠as usando distancia euclidiana\n",
    "    Marca el 5% m√°s lejano como anomal√≠as\n",
    "    \"\"\"\n",
    "    print(\"\\nüî¥ DETECTOR 1: DISTANCIA EUCLIDIANA\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Calcular centroide\n",
    "    centroid = np.mean(X, axis=0)\n",
    "    \n",
    "    # Calcular distancias euclidianas\n",
    "    distances = np.sqrt(np.sum((X - centroid)**2, axis=1))\n",
    "    \n",
    "    # Determinar threshold (percentil 95)\n",
    "    threshold = np.percentile(distances, (1 - contamination) * 100)\n",
    "    \n",
    "    # Identificar anomal√≠as\n",
    "    anomalies = distances > threshold\n",
    "    \n",
    "    # A√±adir resultados al dataframe\n",
    "    data['euclidean_distance'] = distances\n",
    "    data['euclidean_anomaly'] = anomalies\n",
    "    \n",
    "    # Estad√≠sticas\n",
    "    print(f\"üìä Estad√≠sticas:\")\n",
    "    print(f\"   - Distancia promedio: {np.mean(distances):.2f}\")\n",
    "    print(f\"   - Distancia m√°xima: {np.max(distances):.2f}\")\n",
    "    print(f\"   - Threshold (95%): {threshold:.2f}\")\n",
    "    print(f\"   - Anomal√≠as detectadas: {np.sum(anomalies)} ({np.sum(anomalies)/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histograma de distancias\n",
    "    ax1.hist(distances, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "    ax1.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold = {threshold:.2f}')\n",
    "    ax1.set_xlabel('Distancia Euclidiana')\n",
    "    ax1.set_ylabel('Frecuencia')\n",
    "    ax1.set_title('Distribuci√≥n de Distancias Euclidianas')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Serie temporal con anomal√≠as\n",
    "    ax2.scatter(data['hour_rounded'], data['total_ac_power'], \n",
    "                c=data['euclidean_anomaly'], cmap='coolwarm', \n",
    "                alpha=0.6, s=50)\n",
    "    ax2.set_xlabel('Fecha')\n",
    "    ax2.set_ylabel('Potencia AC Total (W)')\n",
    "    ax2.set_title('Anomal√≠as Detectadas - Distancia Euclidiana')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Fechas con anomal√≠as\n",
    "    anomaly_dates = data[data['euclidean_anomaly']]['hour_rounded'].tolist()\n",
    "    \n",
    "    print(f\"\\nüìÖ Primeras 10 fechas con anomal√≠as:\")\n",
    "    for i, date in enumerate(anomaly_dates[:10], 1):\n",
    "        print(f\"   {i}. {date}\")\n",
    "    \n",
    "    return data, anomaly_dates\n",
    "\n",
    "# Aplicar detector euclidiano\n",
    "hourly_data, euclidean_anomalies = euclidean_anomaly_detector(\n",
    "    X_scaled, hourly_data.copy(), contamination=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 9: Detector de Anomal√≠as - Distancia de Mahalanobis\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_anomaly_detector(X, data, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Detecta anomal√≠as usando distancia de Mahalanobis\n",
    "    Considera las correlaciones entre variables\n",
    "    \"\"\"\n",
    "    print(\"\\nüü° DETECTOR 2: DISTANCIA DE MAHALANOBIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Calcular media y covarianza\n",
    "    mean = np.mean(X, axis=0)\n",
    "    \n",
    "    # Usar Minimum Covariance Determinant para robustez\n",
    "    cov_estimator = EmpiricalCovariance()\n",
    "    cov_estimator.fit(X)\n",
    "    \n",
    "    # Calcular distancias de Mahalanobis\n",
    "    mahal_distances = cov_estimator.mahalanobis(X)\n",
    "    \n",
    "    # Determinar threshold usando distribuci√≥n chi-cuadrado\n",
    "    p = X.shape[1]\n",
    "    threshold = chi2.ppf(1 - contamination, df=p)\n",
    "    \n",
    "    # Identificar anomal√≠as\n",
    "    anomalies = mahal_distances > threshold\n",
    "    \n",
    "    # A√±adir resultados\n",
    "    data['mahalanobis_distance'] = mahal_distances\n",
    "    data['mahalanobis_anomaly'] = anomalies\n",
    "    \n",
    "    # Estad√≠sticas\n",
    "    print(f\"üìä Estad√≠sticas:\")\n",
    "    print(f\"   - Dimensiones: {p} variables\")\n",
    "    print(f\"   - Distancia promedio: {np.mean(mahal_distances):.2f}\")\n",
    "    print(f\"   - Distancia m√°xima: {np.max(mahal_distances):.2f}\")\n",
    "    print(f\"   - Threshold œá¬≤(95%): {threshold:.2f}\")\n",
    "    print(f\"   - Anomal√≠as detectadas: {np.sum(anomalies)} ({np.sum(anomalies)/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    # Visualizaci√≥n\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Q-Q plot para verificar distribuci√≥n chi-cuadrado\n",
    "    stats.probplot(mahal_distances, dist=stats.chi2, sparams=(p,), plot=ax1)\n",
    "    ax1.set_title('Q-Q Plot: Mahalanobis vs Chi-cuadrado')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Scatter plot: Euclidiana vs Mahalanobis\n",
    "    ax2.scatter(data['euclidean_distance'], data['mahalanobis_distance'], \n",
    "                c=data['mahalanobis_anomaly'], cmap='viridis', \n",
    "                alpha=0.6, s=50)\n",
    "    ax2.set_xlabel('Distancia Euclidiana')\n",
    "    ax2.set_ylabel('Distancia de Mahalanobis')\n",
    "    ax2.set_title('Comparaci√≥n de Distancias')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # An√°lisis de diferencias con Euclidiana\n",
    "    both_anomalies = data['euclidean_anomaly'] & data['mahalanobis_anomaly']\n",
    "    only_euclidean = data['euclidean_anomaly'] & ~data['mahalanobis_anomaly']\n",
    "    only_mahalanobis = ~data['euclidean_anomaly'] & data['mahalanobis_anomaly']\n",
    "    \n",
    "    print(f\"\\nüîç Comparaci√≥n con Euclidiana:\")\n",
    "    print(f\"   - Anomal√≠as en ambos: {np.sum(both_anomalies)}\")\n",
    "    print(f\"   - Solo Euclidiana: {np.sum(only_euclidean)}\")\n",
    "    print(f\"   - Solo Mahalanobis: {np.sum(only_mahalanobis)}\")\n",
    "    \n",
    "    # Fechas con anomal√≠as\n",
    "    anomaly_dates = data[data['mahalanobis_anomaly']]['hour_rounded'].tolist()\n",
    "    \n",
    "    print(f\"\\nüìÖ Primeras 10 fechas con anomal√≠as (Mahalanobis):\")\n",
    "    for i, date in enumerate(anomaly_dates[:10], 1):\n",
    "        print(f\"   {i}. {date}\")\n",
    "    \n",
    "    return data, anomaly_dates\n",
    "\n",
    "# Aplicar detector Mahalanobis\n",
    "hourly_data, mahalanobis_anomalies = mahalanobis_anomaly_detector(\n",
    "    X_scaled, hourly_data.copy(), contamination=0.05\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 10: Detector de Anomal√≠as - Isolation Forest\n",
    "# ========================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolation_forest_detector(X, data, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Detecta anomal√≠as usando Isolation Forest\n",
    "    Algoritmo basado en √°rboles de decisi√≥n\n",
    "    \"\"\"\n",
    "    print(\"\\nüü¢ DETECTOR 3: ISOLATION FOREST\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Configurar y entrenar Isolation Forest\n",
    "    iso_forest = IsolationForest(\n",
    "        contamination=contamination,\n",
    "        random_state=42,\n",
    "        n_estimators=100,\n",
    "        max_samples='auto',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Predecir anomal√≠as (-1 = anomal√≠a, 1 = normal)\n",
    "    predictions = iso_forest.fit_predict(X)\n",
    "    anomaly_scores = iso_forest.score_samples(X)\n",
    "    \n",
    "    # Convertir a booleano (True = anomal√≠a)\n",
    "    anomalies = predictions == -1\n",
    "    \n",
    "    # A√±adir resultados\n",
    "    data['isolation_score'] = anomaly_scores\n",
    "    data['isolation_anomaly'] = anomalies\n",
    "    \n",
    "    # Estad√≠sticas\n",
    "    print(f\"üìä Estad√≠sticas:\")\n",
    "    print(f\"   - Score promedio: {np.mean(anomaly_scores):.3f}\")\n",
    "    print(f\"   - Score m√≠nimo (m√°s an√≥malo): {np.min(anomaly_scores):.3f}\")\n",
    "    print(f\"   - Anomal√≠as detectadas: {np.sum(anomalies)} ({np.sum(anomalies)/len(data)*100:.1f}%)\")\n",
    "    \n",
    "    # Visualizaci√≥n comparativa\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Distribuci√≥n de scores\n",
    "    axes[0, 0].hist(anomaly_scores, bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('Anomaly Score')\n",
    "    axes[0, 0].set_ylabel('Frecuencia')\n",
    "    axes[0, 0].set_title('Distribuci√≥n de Scores - Isolation Forest')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Comparaci√≥n con otros m√©todos\n",
    "    methods = ['Euclidiana', 'Mahalanobis', 'Isolation Forest']\n",
    "    anomaly_counts = [\n",
    "        np.sum(data['euclidean_anomaly']),\n",
    "        np.sum(data['mahalanobis_anomaly']),\n",
    "        np.sum(data['isolation_anomaly'])\n",
    "    ]\n",
    "    \n",
    "    axes[0, 1].bar(methods, anomaly_counts, color=['red', 'yellow', 'green'], alpha=0.7)\n",
    "    axes[0, 1].set_ylabel('N√∫mero de Anomal√≠as')\n",
    "    axes[0, 1].set_title('Comparaci√≥n de M√©todos')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3. Venn diagram conceptual - Overlap de m√©todos\n",
    "    all_methods = data['euclidean_anomaly'] & data['mahalanobis_anomaly'] & data['isolation_anomaly']\n",
    "    eucl_maha = data['euclidean_anomaly'] & data['mahalanobis_anomaly'] & ~data['isolation_anomaly']\n",
    "    eucl_iso = data['euclidean_anomaly'] & ~data['mahalanobis_anomaly'] & data['isolation_anomaly']\n",
    "    maha_iso = ~data['euclidean_anomaly'] & data['mahalanobis_anomaly'] & data['isolation_anomaly']\n",
    "    \n",
    "    overlap_data = {\n",
    "        'Todos': np.sum(all_methods),\n",
    "        'Eucl+Maha': np.sum(eucl_maha),\n",
    "        'Eucl+Iso': np.sum(eucl_iso),\n",
    "        'Maha+Iso': np.sum(maha_iso),\n",
    "        'Solo Eucl': np.sum(data['euclidean_anomaly'] & ~data['mahalanobis_anomaly'] & ~data['isolation_anomaly']),\n",
    "        'Solo Maha': np.sum(~data['euclidean_anomaly'] & data['mahalanobis_anomaly'] & ~data['isolation_anomaly']),\n",
    "        'Solo Iso': np.sum(~data['euclidean_anomaly'] & ~data['mahalanobis_anomaly'] & data['isolation_anomaly'])\n",
    "    }\n",
    "    \n",
    "    axes[1, 0].bar(overlap_data.keys(), overlap_data.values(), alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Combinaci√≥n de M√©todos')\n",
    "    axes[1, 0].set_ylabel('N√∫mero de Anomal√≠as')\n",
    "    axes[1, 0].set_title('Intersecci√≥n de Detecciones')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Serie temporal con todos los m√©todos\n",
    "    axes[1, 1].scatter(data['hour_rounded'], data['performance_ratio'],\n",
    "                      c='blue', alpha=0.3, s=20, label='Normal')\n",
    "    \n",
    "    # Marcar anomal√≠as de cada m√©todo\n",
    "    for method, color, marker in [\n",
    "        ('euclidean_anomaly', 'red', 'o'),\n",
    "        ('mahalanobis_anomaly', 'yellow', 's'),\n",
    "        ('isolation_anomaly', 'green', '^')\n",
    "    ]:\n",
    "        anomaly_data = data[data[method]]\n",
    "        if len(anomaly_data) > 0:\n",
    "            axes[1, 1].scatter(anomaly_data['hour_rounded'], \n",
    "                              anomaly_data['performance_ratio'],\n",
    "                              c=color, s=50, marker=marker, \n",
    "                              label=method.replace('_anomaly', '').title(),\n",
    "                              edgecolors='black', linewidths=0.5)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Fecha')\n",
    "    axes[1, 1].set_ylabel('Performance Ratio')\n",
    "    axes[1, 1].set_title('Anomal√≠as Detectadas por Cada M√©todo')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Fechas con anomal√≠as\n",
    "    anomaly_dates = data[data['isolation_anomaly']]['hour_rounded'].tolist()\n",
    "    \n",
    "    print(f\"\\nüìÖ Primeras 10 fechas con anomal√≠as (Isolation Forest):\")\n",
    "    for i, date in enumerate(anomaly_dates[:10], 1):\n",
    "        print(f\"   {i}. {date}\")\n",
    "    \n",
    "    # An√°lisis de sensibilidad a correlaciones\n",
    "    only_mahalanobis = np.sum(~data['euclidean_anomaly'] & data['mahalanobis_anomaly'] & ~data['isolation_anomaly'])\n",
    "    print(f\"\\nüîç AN√ÅLISIS DE SENSIBILIDAD A CORRELACIONES:\")\n",
    "    print(f\"\\nMahalanobis detect√≥ {only_mahalanobis} anomal√≠as √∫nicas\")\n",
    "    print(\"Esto sugiere que Mahalanobis es M√ÅS sensible a correlaciones porque:\")\n",
    "    print(\"- Considera la estructura de covarianza de los datos\")\n",
    "    print(\"- Detecta puntos que son an√≥malos en el contexto multivariado\")\n",
    "    print(\"- Identifica observaciones que rompen patrones de correlaci√≥n normales\")\n",
    "    \n",
    "    return data, anomaly_dates\n",
    "\n",
    "# Aplicar Isolation Forest\n",
    "hourly_data, isolation_anomalies = isolation_forest_detector(\n",
    "    X_scaled, hourly_data.copy(), contamination=0.05\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 11: An√°lisis integrado y reporte final\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anomaly_report(data):\n",
    "    \"\"\"\n",
    "    Genera reporte consolidado de anomal√≠as detectadas\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä REPORTE CONSOLIDADO DE ANOMAL√çAS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Crear score de severidad combinado\n",
    "    data['anomaly_count'] = (\n",
    "        data['euclidean_anomaly'].astype(int) +\n",
    "        data['mahalanobis_anomaly'].astype(int) +\n",
    "        data['isolation_anomaly'].astype(int)\n",
    "    )\n",
    "    \n",
    "    # Normalizar scores para severidad (0-100)\n",
    "    data['euclidean_severity'] = (data['euclidean_distance'] / \n",
    "                                  data['euclidean_distance'].max() * 100)\n",
    "    data['mahalanobis_severity'] = (data['mahalanobis_distance'] / \n",
    "                                    data['mahalanobis_distance'].max() * 100)\n",
    "    data['isolation_severity'] = ((1 - (data['isolation_score'] - \n",
    "                                       data['isolation_score'].min()) / \n",
    "                                  (data['isolation_score'].max() - \n",
    "                                   data['isolation_score'].min())) * 100)\n",
    "    \n",
    "    # Score de severidad promedio ponderado\n",
    "    data['severity_score'] = (\n",
    "        0.3 * data['euclidean_severity'] +\n",
    "        0.4 * data['mahalanobis_severity'] +  # Mayor peso por sensibilidad a correlaciones\n",
    "        0.3 * data['isolation_severity']\n",
    "    )\n",
    "    \n",
    "    # Filtrar anomal√≠as (al menos 1 m√©todo)\n",
    "    anomalies = data[data['anomaly_count'] > 0].copy()\n",
    "    anomalies = anomalies.sort_values('severity_score', ascending=False)\n",
    "    \n",
    "    # Top 20 anomal√≠as m√°s severas\n",
    "    print(f\"\\nüö® TOP 20 ANOMAL√çAS M√ÅS SEVERAS:\")\n",
    "    print(f\"{'Fecha':<20} {'M√©todos':<10} {'Severidad':<10} {'PR':<8} {'Efic%':<8} {'Irrad':<8}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for idx, row in anomalies.head(20).iterrows():\n",
    "        methods = row['anomaly_count']\n",
    "        date = row['hour_rounded'].strftime('%Y-%m-%d %H:%M')\n",
    "        severity = row['severity_score']\n",
    "        pr = row['performance_ratio']\n",
    "        eff = row['system_efficiency']\n",
    "        irr = row['poa_irradiance_o_149574']\n",
    "        \n",
    "        print(f\"{date:<20} {methods:<10} {severity:>8.1f} {pr:>8.2f} {eff:>8.1f} {irr:>8.0f}\")\n",
    "    \n",
    "    # Guardar lista completa\n",
    "    anomaly_list = anomalies[['hour_rounded', 'anomaly_count', 'severity_score',\n",
    "                              'euclidean_anomaly', 'mahalanobis_anomaly', \n",
    "                              'isolation_anomaly', 'performance_ratio',\n",
    "                              'system_efficiency', 'total_ac_power']].copy()\n",
    "    \n",
    "    # An√°lisis por m√©todo\n",
    "    print(f\"\\nüìà RESUMEN POR M√âTODO:\")\n",
    "    print(f\"   - Euclidiana: {np.sum(data['euclidean_anomaly'])} anomal√≠as\")\n",
    "    print(f\"   - Mahalanobis: {np.sum(data['mahalanobis_anomaly'])} anomal√≠as\")\n",
    "    print(f\"   - Isolation Forest: {np.sum(data['isolation_anomaly'])} anomal√≠as\")\n",
    "    print(f\"   - Detectadas por 3 m√©todos: {np.sum(data['anomaly_count'] == 3)}\")\n",
    "    print(f\"   - Detectadas por 2 m√©todos: {np.sum(data['anomaly_count'] == 2)}\")\n",
    "    print(f\"   - Detectadas por 1 m√©todo: {np.sum(data['anomaly_count'] == 1)}\")\n",
    "    \n",
    "    # Visualizaci√≥n final\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "    \n",
    "    # Timeline de severidad\n",
    "    scatter = ax1.scatter(data['hour_rounded'], data['severity_score'],\n",
    "                         c=data['anomaly_count'], cmap='YlOrRd', \n",
    "                         s=50, alpha=0.6)\n",
    "    ax1.set_xlabel('Fecha')\n",
    "    ax1.set_ylabel('Score de Severidad (0-100)')\n",
    "    ax1.set_title('Timeline de Anomal√≠as - Severidad y Consenso entre M√©todos')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax1, label='N√∫mero de m√©todos')\n",
    "    \n",
    "    # Distribuci√≥n de severidad por n√∫mero de m√©todos\n",
    "    for i in range(4):\n",
    "        subset = data[data['anomaly_count'] == i]['severity_score']\n",
    "        if len(subset) > 0:\n",
    "            ax2.hist(subset, bins=30, alpha=0.6, \n",
    "                    label=f'{i} m√©todos', density=True)\n",
    "    \n",
    "    ax2.set_xlabel('Score de Severidad')\n",
    "    ax2.set_ylabel('Densidad')\n",
    "    ax2.set_title('Distribuci√≥n de Severidad por Consenso de M√©todos')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return anomaly_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 12: Respuestas finales y guardar resultados\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESPUESTAS A LAS PREGUNTAS DEL AN√ÅLISIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n‚ùì ¬øQu√© m√©todo parece m√°s sensible a las correlaciones?\")\n",
    "print(\"\\n‚úÖ RESPUESTA: La Distancia de Mahalanobis es el m√©todo M√ÅS SENSIBLE a las correlaciones\")\n",
    "print(\"\\nJustificaci√≥n:\")\n",
    "print(\"1. Mahalanobis considera la matriz de covarianza completa de los datos\")\n",
    "print(\"2. Detecta anomal√≠as en el contexto multivariado (relaciones entre variables)\")\n",
    "print(\"3. Euclidiana trata todas las dimensiones independientemente\")\n",
    "print(\"4. Isolation Forest se enfoca en aislamiento estructural, no correlaciones directas\")\n",
    "\n",
    "only_mahalanobis = np.sum(~hourly_data['euclidean_anomaly'] & \n",
    "                          hourly_data['mahalanobis_anomaly'] & \n",
    "                          ~hourly_data['isolation_anomaly'])\n",
    "\n",
    "print(f\"\\nEvidencia: Mahalanobis detect√≥ {only_mahalanobis} anomal√≠as √∫nicas\")\n",
    "print(\"que los otros m√©todos no identificaron, sugiriendo sensibilidad a patrones\")\n",
    "print(\"de correlaci√≥n anormales.\")\n",
    "\n",
    "# Guardar resultados\n",
    "print(\"\\nüíæ Guardando resultados...\")\n",
    "hourly_data.to_csv('anomalias_detectadas_planta_solar.csv', index=False)\n",
    "print(\"‚úÖ Archivo guardado: anomalias_detectadas_planta_solar.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================\n",
    "# CELDA 13: Modelo ensamblado (dise√±o conceptual)\n",
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DISE√ëO DE MODELO ENSAMBLADO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "üìã DIAGRAMA DE FLUJO - MODELO ENSAMBLADO DE DETECCI√ìN DE ANOMAL√çAS\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Datos de       ‚îÇ\n",
    "‚îÇ  Entrada        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "         ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Preprocesamiento‚îÇ\n",
    "‚îÇ - Limpieza      ‚îÇ\n",
    "‚îÇ - Normalizaci√≥n ‚îÇ\n",
    "‚îÇ - Features      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "         ‚îÇ\n",
    "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "    ‚ñº         ‚ñº         ‚ñº          ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n",
    "‚îÇEuclidean‚îÇ‚îÇMahalanobis‚îÇ‚îÇIsolation‚îÇ  ‚îÇ\n",
    "‚îÇDistance ‚îÇ‚îÇDistance   ‚îÇ‚îÇForest   ‚îÇ  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n",
    "     ‚îÇ          ‚îÇ           ‚îÇ       ‚îÇ\n",
    "     ‚ñº          ‚ñº           ‚ñº       ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ    C√°lculo de Scores        ‚îÇ    ‚îÇ\n",
    "‚îÇ  - Score_E (0-1)           ‚îÇ    ‚îÇ\n",
    "‚îÇ  - Score_M (0-1)           ‚îÇ    ‚îÇ\n",
    "‚îÇ  - Score_I (0-1)           ‚îÇ    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "               ‚îÇ                    ‚îÇ\n",
    "               ‚ñº                    ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ  ENSAMBLADO PONDERADO       ‚îÇ    ‚îÇ\n",
    "‚îÇ  Score_final = w1*Score_E + ‚îÇ    ‚îÇ\n",
    "‚îÇ                w2*Score_M + ‚îÇ    ‚îÇ\n",
    "‚îÇ                w3*Score_I   ‚îÇ    ‚îÇ\n",
    "‚îÇ  (w1=0.25, w2=0.45, w3=0.30)‚îÇ    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "               ‚îÇ                    ‚îÇ\n",
    "               ‚ñº                    ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ  M√âTRICA DE SEVERIDAD       ‚îÇ    ‚îÇ\n",
    "‚îÇ  Severity = Score_final √ó F ‚îÇ    ‚îÇ\n",
    "‚îÇ  F = factor contextual      ‚îÇ    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "               ‚îÇ                    ‚îÇ\n",
    "               ‚ñº                    ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ\n",
    "‚îÇ  CLASIFICACI√ìN FINAL        ‚îÇ    ‚îÇ\n",
    "‚îÇ  - Normal (S < 30)         ‚îÇ    ‚îÇ\n",
    "‚îÇ  - Leve (30 ‚â§ S < 50)     ‚îÇ    ‚îÇ\n",
    "‚îÇ  - Moderada (50 ‚â§ S < 70) ‚îÇ    ‚îÇ\n",
    "‚îÇ  - Severa (S ‚â• 70)        ‚îÇ    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ\n",
    "                                   ‚îÇ\n",
    "               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "               ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  RETROALIMENTACI√ìN          ‚îÇ\n",
    "‚îÇ  - Validaci√≥n experta      ‚îÇ\n",
    "‚îÇ  - Ajuste de pesos         ‚îÇ\n",
    "‚îÇ  - Reentrenamiento         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")\n",
    "\n",
    "# Ejemplo de implementaci√≥n\n",
    "def ensemble_anomaly_score(eucl_score, maha_score, iso_score, \n",
    "                          hour, irradiance, affected_inverters):\n",
    "    \"\"\"\n",
    "    Calcula score de anomal√≠a ensamblado con factor contextual\n",
    "    \"\"\"\n",
    "    # Pesos base\n",
    "    w1, w2, w3 = 0.25, 0.45, 0.30\n",
    "    \n",
    "    # Score ponderado base\n",
    "    base_score = w1 * eucl_score + w2 * maha_score + w3 * iso_score\n",
    "    \n",
    "    # Factor contextual\n",
    "    night_factor = 2.0 if hour < 6 or hour > 20 else 1.0\n",
    "    low_irr_factor = 1.5 if irradiance < 100 else 1.0\n",
    "    multi_inv_factor = 1 + (affected_inverters / 24)\n",
    "    \n",
    "    # Score final\n",
    "    contextual_factor = night_factor * low_irr_factor * multi_inv_factor\n",
    "    final_score = min(100, base_score * contextual_factor)\n",
    "    \n",
    "    return final_score\n",
    "\n",
    "print(\"\\n‚úÖ An√°lisis completado exitosamente!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
