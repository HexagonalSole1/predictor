{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔋 Sistema Integrado de Detección de Anomalías en Paneles Solares\n",
    "## Versión Integrada que usa todos los módulos desarrollados\n",
    "\n",
    "**Características:**\n",
    "- ✅ Usa `mejoras_ingenieria_caracteristicas.py`\n",
    "- ✅ Usa `mejoras_modelo_anomalias.py`\n",
    "- ✅ Usa `sistema_mejorado_completo.py`\n",
    "- ✅ Mantiene análisis Prophet original\n",
    "- ✅ Sistema de alertas integrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Módulos propios importados correctamente\n",
      "✅ Prophet importado correctamente\n",
      "\n",
      "🚀 SISTEMA INTEGRADO DE DETECCIÓN DE ANOMALÍAS\n",
      "============================================================\n",
      "✓ Librerías básicas importadas\n",
      "✓ Módulos propios: Disponibles\n",
      "✓ Prophet: Disponible\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELDA 1: Importaciones y configuración\n",
    "# ========================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ========================================\n",
    "# IMPORTAR NUESTROS MÓDULOS DESARROLLADOS\n",
    "# ========================================\n",
    "try:\n",
    "    from mejoras_ingenieria_caracteristicas import MejorasIngenieriaCaracteristicas\n",
    "    from mejoras_modelo_anomalias import MejorasModeloAnomalias\n",
    "    from sistema_mejorado_completo import SistemaMejoradoCompleto\n",
    "    print(\"✅ Módulos propios importados correctamente\")\n",
    "    modulos_disponibles = True\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Error importando módulos: {e}\")\n",
    "    print(\"Continuando con funcionalidad básica...\")\n",
    "    modulos_disponibles = False\n",
    "\n",
    "# Prophet para análisis predictivo\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    print(\"✅ Prophet importado correctamente\")\n",
    "    prophet_disponible = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ Prophet no disponible\")\n",
    "    prophet_disponible = False\n",
    "\n",
    "print(\"\\n🚀 SISTEMA INTEGRADO DE DETECCIÓN DE ANOMALÍAS\")\n",
    "print(\"=\"*60)\n",
    "print(\"✓ Librerías básicas importadas\")\n",
    "print(f\"✓ Módulos propios: {'Disponibles' if modulos_disponibles else 'No disponibles'}\")\n",
    "print(f\"✓ Prophet: {'Disponible' if prophet_disponible else 'No disponible'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELDA 2: Funciones de carga de datos\n",
    "# ========================================\n",
    "\n",
    "def find_csv_files(directorio_raiz='.'):\n",
    "    \"\"\"Buscar archivos CSV en el directorio\"\"\"\n",
    "    directorios_con_csv = []\n",
    "    for raiz, directorios, archivos in os.walk(directorio_raiz):\n",
    "        archivos_csv = [archivo for archivo in archivos if archivo.lower().endswith('.csv')]\n",
    "        if archivos_csv:\n",
    "            directorios_con_csv.append(raiz)\n",
    "    return directorios_con_csv\n",
    "\n",
    "def cargar_datasets():\n",
    "    \"\"\"Función mejorada para cargar los 3 datasets\"\"\"\n",
    "    print(\"📊 CARGANDO DATASETS...\")\n",
    "    \n",
    "    directorios = find_csv_files()\n",
    "    print(\"Directorios que contienen archivos CSV:\")\n",
    "    for directorio in directorios:\n",
    "        print(f\"- {directorio}\")\n",
    "    \n",
    "    # Buscar los archivos específicos\n",
    "    environment_data = None\n",
    "    irradiance_data = None\n",
    "    electrical_data = None\n",
    "    \n",
    "    for directorio in directorios:\n",
    "        # Environment data\n",
    "        ruta_env = os.path.join(directorio, 'environment_data.csv')\n",
    "        if os.path.exists(ruta_env):\n",
    "            environment_data = pd.read_csv(ruta_env)\n",
    "            print(f\"✓ Cargado: {ruta_env}\")\n",
    "        \n",
    "        # Irradiance data\n",
    "        ruta_irr = os.path.join(directorio, 'irradiance_data.csv')\n",
    "        if os.path.exists(ruta_irr):\n",
    "            irradiance_data = pd.read_csv(ruta_irr)\n",
    "            print(f\"✓ Cargado: {ruta_irr}\")\n",
    "        \n",
    "        # Electrical data (inversores)\n",
    "        ruta_elec = os.path.join(directorio, 'electrical_data.csv')\n",
    "        if os.path.exists(ruta_elec):\n",
    "            electrical_data = pd.read_csv(ruta_elec)\n",
    "            print(f\"✓ Cargado: {ruta_elec}\")\n",
    "        \n",
    "        # También buscar chunk_electrical_data.csv como alternativa\n",
    "        ruta_chunk = os.path.join(directorio, 'chunk_electrical_data.csv')\n",
    "        if os.path.exists(ruta_chunk) and electrical_data is None:\n",
    "            electrical_data = pd.read_csv(ruta_chunk)\n",
    "            print(f\"✓ Cargado: {ruta_chunk}\")\n",
    "    \n",
    "    return environment_data, irradiance_data, electrical_data\n",
    "\n",
    "def seleccionar_variables_inversor_1(electrical_data):\n",
    "    \"\"\"Seleccionar solo las variables del inversor 1 y limpiar nombres\"\"\"\n",
    "    # Seleccionar columnas del inversor 1\n",
    "    columnas_inv1 = ['measured_on']\n",
    "    for col in electrical_data.columns:\n",
    "        if 'inv_01_' in col:\n",
    "            columnas_inv1.append(col)\n",
    "    \n",
    "    df_inv1 = electrical_data[columnas_inv1].copy()\n",
    "    \n",
    "    # Limpiar nombres de columnas (quitar el sufijo _inv_XXXXX)\n",
    "    columnas_limpias = {}\n",
    "    for columna in df_inv1.columns:\n",
    "        if columna == 'measured_on':\n",
    "            columnas_limpias[columna] = columna\n",
    "        elif columna.startswith('inv_01_'):\n",
    "            # Extraer solo la parte del tipo de medición\n",
    "            partes = columna.split('_inv_')\n",
    "            if len(partes) >= 1:\n",
    "                nuevo_nombre = partes[0].replace('inv_01_', '')\n",
    "                columnas_limpias[columna] = nuevo_nombre\n",
    "    \n",
    "    df_inv1 = df_inv1.rename(columns=columnas_limpias)\n",
    "    return df_inv1\n",
    "\n",
    "def combinar_datasets(environment_data, irradiance_data, electrical_data):\n",
    "    \"\"\"Realizar INNER JOIN de los 3 datasets por measured_on\"\"\"\n",
    "    # Seleccionar variables del inversor 1\n",
    "    df_inv1 = seleccionar_variables_inversor_1(electrical_data)\n",
    "    \n",
    "    print(\"Variables seleccionadas del inversor 1:\")\n",
    "    print(df_inv1.columns.tolist())\n",
    "    \n",
    "    # Convertir measured_on a datetime en todos los datasets\n",
    "    environment_data['measured_on'] = pd.to_datetime(environment_data['measured_on'])\n",
    "    irradiance_data['measured_on'] = pd.to_datetime(irradiance_data['measured_on'])\n",
    "    df_inv1['measured_on'] = pd.to_datetime(df_inv1['measured_on'])\n",
    "    \n",
    "    # Realizar INNER JOIN\n",
    "    print(\"\\n📊 Realizando INNER JOIN...\")\n",
    "    df_combined = environment_data.merge(irradiance_data, on='measured_on', how='inner')\n",
    "    df_final = df_combined.merge(df_inv1, on='measured_on', how='inner')\n",
    "    \n",
    "    print(f\"✓ Dataset final: {len(df_final)} filas, {len(df_final.columns)} columnas\")\n",
    "    print(\"Columnas finales:\", df_final.columns.tolist())\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 CARGANDO DATASETS...\n",
      "Directorios que contienen archivos CSV:\n",
      "- .\n",
      "✓ Cargado: ./environment_data.csv\n",
      "✓ Cargado: ./irradiance_data.csv\n",
      "✓ Cargado: ./electrical_data.csv\n",
      "Variables seleccionadas del inversor 1:\n",
      "['measured_on', 'dc_current', 'dc_voltage', 'ac_current', 'ac_voltage', 'ac_power']\n",
      "\n",
      "📊 Realizando INNER JOIN...\n",
      "✓ Dataset final: 175566 filas, 10 columnas\n",
      "Columnas finales: ['measured_on', 'ambient_temperature_o_149575', 'wind_speed_o_149576', 'wind_direction_o_149577', 'poa_irradiance_o_149574', 'dc_current', 'dc_voltage', 'ac_current', 'ac_voltage', 'ac_power']\n",
      "\n",
      "✅ Datos combinados exitosamente: (175566, 10)\n",
      "\n",
      "📊 INFORMACIÓN DEL DATASET COMBINADO:\n",
      "   - Registros: 175,566\n",
      "   - Columnas: 10\n",
      "   - Período: 2017-12-01 00:15:00 a 2023-10-31 23:45:00\n",
      "   - Valores nulos: 143\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELDA 3: Cargar datos\n",
    "# ========================================\n",
    "\n",
    "# Cargar los datasets\n",
    "environment_data, irradiance_data, electrical_data = cargar_datasets()\n",
    "\n",
    "if any(data is None for data in [environment_data, irradiance_data, electrical_data]):\n",
    "    print(\"❌ Error: No se pudieron cargar todos los datasets necesarios\")\n",
    "    print(\"Archivos requeridos:\")\n",
    "    print(\"- environment_data.csv\")\n",
    "    print(\"- irradiance_data.csv\")\n",
    "    print(\"- electrical_data.csv (o chunk_electrical_data.csv)\")\n",
    "else:\n",
    "    # Combinar datasets\n",
    "    df_combined = combinar_datasets(environment_data, irradiance_data, electrical_data)\n",
    "    print(f\"\\n✅ Datos combinados exitosamente: {df_combined.shape}\")\n",
    "    \n",
    "    # Mostrar información básica\n",
    "    print(f\"\\n📊 INFORMACIÓN DEL DATASET COMBINADO:\")\n",
    "    print(f\"   - Registros: {len(df_combined):,}\")\n",
    "    print(f\"   - Columnas: {len(df_combined.columns)}\")\n",
    "    print(f\"   - Período: {df_combined['measured_on'].min()} a {df_combined['measured_on'].max()}\")\n",
    "    print(f\"   - Valores nulos: {df_combined.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 APLICANDO INGENIERÍA DE CARACTERÍSTICAS CON MÓDULO INTEGRADO\n",
      "======================================================================\n",
      "Aplicando mejoras de ingeniería de características...\n",
      "🚀 APLICANDO TODAS LAS MEJORAS DE INGENIERÍA DE CARACTERÍSTICAS\n",
      "===========================================================================\n",
      "🕐 CREANDO CARACTERÍSTICAS TEMPORALES AVANZADAS\n",
      "============================================================\n",
      "✓ Características temporales creadas: 14 nuevas variables\n",
      "\n",
      "⚡ CREANDO CARACTERÍSTICAS FÍSICAS MEJORADAS\n",
      "=======================================================\n",
      "⚠️ Error en ingeniería de características: 'total_ac_power'\n",
      "Continuando con características básicas...\n",
      "Creando características básicas...\n",
      "✓ Características básicas creadas: 14 columnas total\n",
      "\n",
      "📊 RESUMEN DE CARACTERÍSTICAS:\n",
      "   - Dataset shape: (175566, 14)\n",
      "   - Tipo de características: Básicas\n",
      "   - Registros válidos: 175566\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELDA 4: INGENIERÍA DE CARACTERÍSTICAS USANDO MÓDULO\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n🔧 APLICANDO INGENIERÍA DE CARACTERÍSTICAS CON MÓDULO INTEGRADO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Preparar datos con índice temporal\n",
    "df_for_features = df_combined.copy()\n",
    "df_for_features = df_for_features.set_index('measured_on')\n",
    "\n",
    "if modulos_disponibles:\n",
    "    # Usar el módulo de mejoras de características\n",
    "    try:\n",
    "        # Inicializar el mejorador de características\n",
    "        mejorador_caracteristicas = MejorasIngenieriaCaracteristicas()\n",
    "        \n",
    "        # Aplicar todas las mejoras de características\n",
    "        print(\"Aplicando mejoras de ingeniería de características...\")\n",
    "        df_con_caracteristicas = mejorador_caracteristicas.aplicar_todas_mejoras(df_for_features)\n",
    "        \n",
    "        print(f\"✅ Ingeniería de características completada exitosamente\")\n",
    "        print(f\"   - Características originales: {len(df_for_features.columns)}\")\n",
    "        print(f\"   - Características finales: {len(df_con_caracteristicas.columns)}\")\n",
    "        print(f\"   - Nuevas características creadas: {len(df_con_caracteristicas.columns) - len(df_for_features.columns)}\")\n",
    "        \n",
    "        caracteristicas_creadas = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error en ingeniería de características: {e}\")\n",
    "        print(\"Continuando con características básicas...\")\n",
    "        df_con_caracteristicas = df_for_features.copy()\n",
    "        caracteristicas_creadas = False\n",
    "else:\n",
    "    print(\"⚠️ Módulo de características no disponible, usando características básicas\")\n",
    "    df_con_caracteristicas = df_for_features.copy()\n",
    "    caracteristicas_creadas = False\n",
    "\n",
    "# Crear algunas características básicas si el módulo no está disponible\n",
    "if not caracteristicas_creadas:\n",
    "    print(\"Creando características básicas...\")\n",
    "    \n",
    "    # Características temporales básicas\n",
    "    df_con_caracteristicas['hour'] = df_con_caracteristicas.index.hour\n",
    "    df_con_caracteristicas['day_of_week'] = df_con_caracteristicas.index.dayofweek\n",
    "    df_con_caracteristicas['month'] = df_con_caracteristicas.index.month\n",
    "    \n",
    "    # Potencia total AC si hay múltiples columnas\n",
    "    ac_power_cols = [col for col in df_con_caracteristicas.columns if 'ac_power' in col]\n",
    "    if len(ac_power_cols) > 1:\n",
    "        df_con_caracteristicas['total_ac_power'] = df_con_caracteristicas[ac_power_cols].sum(axis=1)\n",
    "    elif len(ac_power_cols) == 1:\n",
    "        df_con_caracteristicas['total_ac_power'] = df_con_caracteristicas[ac_power_cols[0]]\n",
    "    \n",
    "    # Eficiencia básica\n",
    "    if 'total_ac_power' in df_con_caracteristicas.columns and 'poa_irradiance_o_149574' in df_con_caracteristicas.columns:\n",
    "        df_con_caracteristicas['eficiencia_basica'] = df_con_caracteristicas['total_ac_power'] / (df_con_caracteristicas['poa_irradiance_o_149574'] + 1e-6)\n",
    "    \n",
    "    print(f\"✓ Características básicas creadas: {len(df_con_caracteristicas.columns)} columnas total\")\n",
    "\n",
    "print(f\"\\n📊 RESUMEN DE CARACTERÍSTICAS:\")\n",
    "print(f\"   - Dataset shape: {df_con_caracteristicas.shape}\")\n",
    "print(f\"   - Tipo de características: {'Avanzadas (95+)' if caracteristicas_creadas else 'Básicas'}\")\n",
    "print(f\"   - Registros válidos: {len(df_con_caracteristicas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏷️ CREANDO ETIQUETAS DE ANOMALÍA\n",
      "=============================================\n",
      "   - Usando 14 características numéricas\n",
      "   - Datos limpios: 175566 registros\n",
      "   - Aplicando Isolation Forest...\n",
      "   - Aplicando Local Outlier Factor...\n",
      "   - Isolation Forest detectó: 8779 anomalías\n",
      "   - LOF detectó: 7461 anomalías\n",
      "   - Consenso final: 15649 anomalías\n",
      "\n",
      "✅ Etiquetas de anomalía creadas exitosamente\n",
      "   - Anomalías detectadas: 15649 (8.91%)\n",
      "   - Datos normales: 159917 (91.09%)\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELDA 5: CREAR ETIQUETAS DE ANOMALÍA\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n🏷️ CREANDO ETIQUETAS DE ANOMALÍA\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "def crear_etiquetas_anomalia_basicas(df):\n",
    "    \"\"\"Crear etiquetas de anomalía usando métodos básicos\"\"\"\n",
    "    # Usar métodos simples para crear etiquetas\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    \n",
    "    # Seleccionar características numéricas\n",
    "    numeric_data = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Limpiar datos\n",
    "    numeric_data = numeric_data.fillna(numeric_data.median())\n",
    "    numeric_data = numeric_data.replace([np.inf, -np.inf], np.nan)\n",
    "    numeric_data = numeric_data.fillna(numeric_data.median())\n",
    "    \n",
    "    print(f\"   - Usando {len(numeric_data.columns)} características numéricas\")\n",
    "    print(f\"   - Datos limpios: {len(numeric_data)} registros\")\n",
    "    \n",
    "    # Normalizar\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(numeric_data)\n",
    "    \n",
    "    # Isolation Forest\n",
    "    print(\"   - Aplicando Isolation Forest...\")\n",
    "    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "    anomalias_iso = iso_forest.fit_predict(X_scaled)\n",
    "    \n",
    "    # LOF\n",
    "    print(\"   - Aplicando Local Outlier Factor...\")\n",
    "    lof = LocalOutlierFactor(contamination=0.05, novelty=True)\n",
    "    lof.fit(X_scaled)\n",
    "    anomalias_lof = lof.predict(X_scaled)\n",
    "    \n",
    "    # Combinar (anomalía si al menos uno la detecta)\n",
    "    etiquetas_finales = ((anomalias_iso == -1) | (anomalias_lof == -1)).astype(int)\n",
    "    \n",
    "    print(f\"   - Isolation Forest detectó: {(anomalias_iso == -1).sum()} anomalías\")\n",
    "    print(f\"   - LOF detectó: {(anomalias_lof == -1).sum()} anomalías\")\n",
    "    print(f\"   - Consenso final: {etiquetas_finales.sum()} anomalías\")\n",
    "    \n",
    "    return etiquetas_finales\n",
    "\n",
    "# Crear etiquetas\n",
    "try:\n",
    "    etiquetas_anomalia = crear_etiquetas_anomalia_basicas(df_con_caracteristicas)\n",
    "    df_con_caracteristicas['anomalia'] = etiquetas_anomalia\n",
    "    \n",
    "    print(f\"\\n✅ Etiquetas de anomalía creadas exitosamente\")\n",
    "    print(f\"   - Anomalías detectadas: {etiquetas_anomalia.sum()} ({etiquetas_anomalia.sum()/len(etiquetas_anomalia)*100:.2f}%)\")\n",
    "    print(f\"   - Datos normales: {(etiquetas_anomalia == 0).sum()} ({(etiquetas_anomalia == 0).sum()/len(etiquetas_anomalia)*100:.2f}%)\")\n",
    "    \n",
    "    etiquetas_creadas = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error creando etiquetas: {e}\")\n",
    "    # Crear etiquetas dummy si hay error\n",
    "    df_con_caracteristicas['anomalia'] = 0\n",
    "    etiquetas_creadas = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 APLICANDO MEJORAS DE MODELOS ML CON MÓDULO INTEGRADO\n",
      "=================================================================\n",
      "✓ Datos preparados para ML:\n",
      "   - Características: 14\n",
      "   - Muestras: 175566\n",
      "   - Anomalías: 15649\n",
      "Aplicando mejoras de modelo (supervisado)...\n",
      "🚀 APLICANDO TODAS LAS MEJORAS DEL MODELO\n",
      "=======================================================\n",
      "\n",
      "📏 CREANDO ESCALADORES ROBUSTOS\n",
      "========================================\n",
      "\n",
      "🧠 APLICANDO ESCALADO INTELIGENTE (auto)\n",
      "==================================================\n",
      "✓ Escalado aplicado: robust\n",
      "✓ Forma de datos: (175566, 14)\n",
      "🤖 CREANDO ENSEMBLE AVANZADO DE MODELOS\n",
      "=======================================================\n",
      "\n",
      "🔧 OPTIMIZANDO HIPERPARÁMETROS PARA RANDOM_FOREST\n",
      "============================================================\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# CELDA 6: APLICAR MEJORAS DE MODELO ML USANDO MÓDULO\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n🤖 APLICANDO MEJORAS DE MODELOS ML CON MÓDULO INTEGRADO\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "if modulos_disponibles and etiquetas_creadas:\n",
    "    # Preparar datos para ML\n",
    "    X_features = df_con_caracteristicas.select_dtypes(include=[np.number]).drop('anomalia', axis=1, errors='ignore')\n",
    "    y_labels = df_con_caracteristicas['anomalia']\n",
    "    \n",
    "    # Limpiar datos finales\n",
    "    X_features = X_features.fillna(X_features.median())\n",
    "    X_features = X_features.replace([np.inf, -np.inf], np.nan)\n",
    "    X_features = X_features.fillna(X_features.median())\n",
    "    \n",
    "    print(f\"✓ Datos preparados para ML:\")\n",
    "    print(f\"   - Características: {X_features.shape[1]}\")\n",
    "    print(f\"   - Muestras: {X_features.shape[0]}\")\n",
    "    print(f\"   - Anomalías: {y_labels.sum()}\")\n",
    "    \n",
    "    # Verificar que tenemos suficientes anomalías para entrenamiento\n",
    "    if y_labels.sum() < 5:\n",
    "        print(\"⚠️ Muy pocas anomalías para entrenamiento supervisado\")\n",
    "        print(\"Usando método no supervisado...\")\n",
    "        metodo_ml = 'no_supervisado'\n",
    "    else:\n",
    "        metodo_ml = 'supervisado'\n",
    "    \n",
    "    try:\n",
    "        # Inicializar mejorador de modelos\n",
    "        mejorador_modelo = MejorasModeloAnomalias()\n",
    "        \n",
    "        # Aplicar todas las mejoras de modelo\n",
    "        print(f\"Aplicando mejoras de modelo ({metodo_ml})...\")\n",
    "        resultados_ml = mejorador_modelo.aplicar_todas_mejoras(\n",
    "            X_features, \n",
    "            y_labels if metodo_ml == 'supervisado' else None, \n",
    "            metodo=metodo_ml\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Mejoras de modelo aplicadas exitosamente\")\n",
    "        print(f\"   - Modelos entrenados: {len(resultados_ml['modelos'])}\")\n",
    "        print(f\"   - Método usado: {metodo_ml}\")\n",
    "        \n",
    "        if 'sistema_multinivel' in resultados_ml:\n",
    "            print(f\"   - Sistema multinivel: {len(resultados_ml['sistema_multinivel'])} niveles\")\n",
    "        \n",
    "        # Evaluar modelos si es supervisado\n",
    "        if metodo_ml == 'supervisado':\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            \n",
    "            try:\n",
    "                X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X_features, y_labels, test_size=0.2, random_state=42, stratify=y_labels\n",
    "                )\n",
    "                \n",
    "                # Evaluar cada modelo\n",
    "                resultados_evaluacion = {}\n",
    "                print(\"\\n📊 Evaluando modelos:\")\n",
    "                \n",
    "                for nombre, modelo in resultados_ml['modelos'].items():\n",
    "                    try:\n",
    "                        modelo.fit(X_train, y_train)\n",
    "                        resultado_eval = mejorador_modelo.evaluar_modelo_avanzado(\n",
    "                            modelo, X_test, y_test, nombre\n",
    "                        )\n",
    "                        resultados_evaluacion[nombre] = resultado_eval\n",
    "                        print(f\"   - {nombre}: F1-Score = {resultado_eval['f1']:.3f}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"   - Error evaluando {nombre}: {e}\")\n",
    "                \n",
    "                # Encontrar mejor modelo\n",
    "                if resultados_evaluacion:\n",
    "                    mejor_modelo = max(resultados_evaluacion.keys(), \n",
    "                                      key=lambda x: resultados_evaluacion[x]['f1'])\n",
    "                    print(f\"\\n🏆 Mejor modelo: {mejor_modelo} (F1: {resultados_evaluacion[mejor_modelo]['f1']:.3f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error en evaluación: {e}\")\n",
    "                resultados_evaluacion = None\n",
    "        else:\n",
    "            resultados_evaluacion = None\n",
    "        \n",
    "        ml_exitoso = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error en mejoras de modelo: {e}\")\n",
    "        print(\"Continuando con análisis básico...\")\n",
    "        resultados_ml = None\n",
    "        resultados_evaluacion = None\n",
    "        ml_exitoso = False\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Módulos ML no disponibles o etiquetas no creadas\")\n",
    "    print(\"Saltando análisis ML avanzado...\")\n",
    "    resultados_ml = None\n",
    "    resultados_evaluacion = None\n",
    "    ml_exitoso = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELDA 7: ANÁLISIS PROPHET (PRESERVADO DEL ORIGINAL)\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n🔮 ANÁLISIS PREDICTIVO CON PROPHET\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "def analisis_prophet_integrado(df_original):\n",
    "    \"\"\"Análisis Prophet integrado con el sistema\"\"\"\n",
    "    \n",
    "    if not prophet_disponible:\n",
    "        print(\"⚠️ Prophet no está disponible\")\n",
    "        return None\n",
    "    \n",
    "    # Preparar datos para Prophet\n",
    "    df_prophet = df_original.copy()\n",
    "    \n",
    "    # Buscar columna de potencia AC\n",
    "    power_cols = [col for col in df_prophet.columns if 'ac_power' in col.lower()]\n",
    "    if not power_cols:\n",
    "        # Crear potencia total si no existe\n",
    "        ac_cols = [col for col in df_prophet.columns if 'ac_power' in col]\n",
    "        if ac_cols:\n",
    "            df_prophet['total_ac_power'] = df_prophet[ac_cols].sum(axis=1)\n",
    "            power_col = 'total_ac_power'\n",
    "        else:\n",
    "            print(\"⚠️ No se encontraron columnas de potencia AC\")\n",
    "            return None\n",
    "    else:\n",
    "        power_col = power_cols[0]\n",
    "    \n",
    "    print(f\"Usando columna de potencia: {power_col}\")\n",
    "    \n",
    "    # Preparar datos\n",
    "    df_prophet = df_prophet[['measured_on', power_col]].dropna()\n",
    "    df_prophet = df_prophet[df_prophet[power_col] > 0]\n",
    "    \n",
    "    print(f\"Datos iniciales para Prophet: {len(df_prophet)} registros\")\n",
    "    \n",
    "    # Filtro IQR para limpiar outliers extremos\n",
    "    Q1 = df_prophet[power_col].quantile(0.25)\n",
    "    Q3 = df_prophet[power_col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df_prophet = df_prophet[\n",
    "        (df_prophet[power_col] >= Q1 - 1.0 * IQR) & \n",
    "        (df_prophet[power_col] <= Q3 + 1.0 * IQR)\n",
    "    ]\n",
    "    \n",
    "    # Filtro por percentiles extremos\n",
    "    lower = df_prophet[power_col].quantile(0.02)\n",
    "    upper = df_prophet[power_col].quantile(0.98)\n",
    "    df_prophet = df_prophet[\n",
    "        (df_prophet[power_col] >= lower) & \n",
    "        (df_prophet[power_col] <= upper)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Datos después de filtros: {len(df_prophet)} registros\")\n",
    "    \n",
    "    # Resampleo diario\n",
    "    df_hourly = df_prophet.set_index('measured_on').resample('D').mean().dropna().reset_index()\n",
    "    \n",
    "    # Preparar para Prophet\n",
    "    df_prophet_final = df_hourly.rename(columns={'measured_on': 'ds', power_col: 'y'})[['ds', 'y']]\n",
    "    df_prophet_final = df_prophet_final[df_prophet_final['y'] > 5]\n",
    "    \n",
    "    if len(df_prophet_final) < 30:\n",
    "        print(f\"⚠️ Datos insuficientes para Prophet: {len(df_prophet_final)} < 30\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"✓ Datos finales para Prophet: {len(df_prophet_final)} registros\")\n",
    "    \n",
    "    try:\n",
    "        # Entrenar modelo Prophet\n",
    "        print(\"Entrenando modelo Prophet...\")\n",
    "        model = Prophet(\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=False,\n",
    "            seasonality_mode='additive'\n",
    "        )\n",
    "        model.fit(df_prophet_final)\n",
    "        \n",
    "        # Predicciones futuras\n",
    "        print(\"Generando predicciones futuras...\")\n",
    "        future = model.make_future_dataframe(periods=360)\n",
    "        forecast = model.predict(future)\n",
    "        \n",
    "        # Evitar valores negativos\n",
    "        forecast['yhat'] = forecast['yhat'].clip(lower=0)\n",
    "        forecast['yhat_lower'] = forecast['yhat_lower'].clip(lower=0)\n",
    "        forecast['yhat_upper'] = forecast['yhat_upper'].clip(lower=0)\n",
    "        \n",
    "        # Evaluación del modelo\n",
    "        y_true = df_prophet_final['y'].reset_index(drop=True)\n",
    "        y_pred = forecast['yhat'][:len(y_true)].reset_index(drop=True)\n",
    "        \n",
    "        from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        \n",
    "        # SMAPE\n",
    "        def smape(y_true, y_pred):\n",
    "            return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "        \n",
    "        smape_val = smape(y_true, y_pred)\n",
    "        \n",
    "        print(f\"✅ Modelo Prophet entrenado exitosamente:\")\n",
    "        print(f\"   - MAE: {mae:.2f}\")\n",
    "        print(f\"   - RMSE: {rmse:.2f}\")\n",
    "        print(f\"   - SMAPE: {smape_val:.2f}%\")\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'forecast': forecast,\n",
    "            'data_used': df_prophet_final,\n",
    "            'metrics': {\n",
    "                'mae': mae,\n",
    "                'rmse': rmse,\n",
    "                'smape': smape_val\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error en Prophet: {e}\")\n",
    "        return None\n",
    "\n",
    "# Ejecutar análisis Prophet\n",
    "resultados_prophet = analisis_prophet_integrado(df_combined)\n",
    "\n",
    "if resultados_prophet:\n",
    "    print(f\"\\n✅ Análisis Prophet completado exitosamente\")\n",
    "    print(f\"   - Predicciones generadas: {len(resultados_prophet['forecast'])}\")\n",
    "    print(f\"   - Datos usados: {len(resultados_prophet['data_used'])}\")\n",
    "    print(f\"   - SMAPE: {resultados_prophet['metrics']['smape']:.2f}%\")\n",
    "else:\n",
    "    print(f\"⚠️ No se pudo completar el análisis Prophet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELDA 8: VISUALIZACIONES INTEGRADAS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n📊 GENERANDO VISUALIZACIONES INTEGRADAS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def visualizaciones_integradas(df_anomalias, resultados_ml, resultados_prophet, resultados_eval):\n",
    "    \"\"\"Crear visualizaciones que integran todos los resultados\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    fig.suptitle('🔋 Dashboard Integrado - Sistema de Detección de Anomalías', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Prophet - Predicciones (si disponible)\n",
    "    ax1 = axes[0, 0]\n",
    "    if resultados_prophet:\n",
    "        prophet_data = resultados_prophet['data_used']\n",
    "        forecast = resultados_prophet['forecast']\n",
    "        \n",
    "        ax1.plot(prophet_data['ds'], prophet_data['y'], 'o', markersize=3, label='Datos Reales', alpha=0.7, color='blue')\n",
    "        ax1.plot(forecast['ds'], forecast['yhat'], '-', color='red', label='Predicción Prophet', linewidth=2)\n",
    "        ax1.fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'], \n",
    "                        color='red', alpha=0.2, label='Intervalo de Confianza')\n",
    "        ax1.set_title('🔮 Predicciones Prophet - Producción Solar')\n",
    "        ax1.set_xlabel('Fecha')\n",
    "        ax1.set_ylabel('Potencia AC')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, 'Prophet no disponible', transform=ax1.transAxes, \n",
    "                ha='center', va='center', fontsize=14)\n",
    "        ax1.set_title('🔮 Predicciones Prophet - No Disponible')\n",
    "    \n",
    "    # 2. Rendimiento de modelos ML (si disponible)\n",
    "    ax2 = axes[0, 1]\n",
    "    if resultados_eval and len(resultados_eval) > 0:\n",
    "        modelos = list(resultados_eval.keys())\n",
    "        f1_scores = [resultados_eval[modelo]['f1'] for modelo in modelos]\n",
    "        \n",
    "        colors = ['red', 'orange', 'green', 'blue', 'purple'][:len(modelos)]\n",
    "        bars = ax2.bar(modelos, f1_scores, color=colors)\n",
    "        ax2.set_title('🤖 Rendimiento de Modelos ML (F1-Score)')\n",
    "        ax2.set_ylabel('F1-Score')\n",
    "        ax2.set_ylim(0, 1)\n",
    "        \n",
    "        # Agregar valores en las barras\n",
    "        for bar, score in zip(bars, f1_scores):\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{score:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Modelos ML no evaluados', transform=ax2.transAxes, \n",
    "                ha='center', va='center', fontsize=14)\n",
    "        ax2.set_title('🤖 Modelos ML - No Disponible')\n",
    "    \n",
    "    # 3. Distribución de anomalías en el tiempo\n",
    "    ax3 = axes[1, 0]\n",
    "    if 'anomalia' in df_anomalias.columns:\n",
    "        # Crear serie temporal de anomalías\n",
    "        df_time_anomalies = df_anomalias.copy()\n",
    "        if hasattr(df_time_anomalies.index, 'date'):\n",
    "            # Resamplear por día para ver patrones\n",
    "            daily_anomalies = df_time_anomalies['anomalia'].resample('D').sum()\n",
    "            \n",
    "            ax3.plot(daily_anomalies.index, daily_anomalies.values, 'r-', alpha=0.7, linewidth=2)\n",
    "            ax3.fill_between(daily_anomalies.index, daily_anomalies.values, alpha=0.3, color='red')\n",
    "            ax3.set_title('📅 Anomalías Detectadas por Día')\n",
    "            ax3.set_xlabel('Fecha')\n",
    "            ax3.set_ylabel('Número de Anomalías')\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            # Gráfico de barras simple si no hay índice temporal\n",
    "            anomalias_count = df_anomalias['anomalia'].value_counts()\n",
    "            ax3.bar(['Normal', 'Anomalía'], [anomalias_count.get(0, 0), anomalias_count.get(1, 0)], \n",
    "                   color=['green', 'red'], alpha=0.7)\n",
    "            ax3.set_title('📊 Distribución de Anomalías')\n",
    "            ax3.set_ylabel('Cantidad')\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'Anomalías no disponibles', transform=ax3.transAxes, \n",
    "                ha='center', va='center', fontsize=14)\n",
    "        ax3.set_title('📅 Anomalías - No Disponible')\n",
    "    \n",
    "    # 4. Estadísticas del dataset\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    # Mostrar estadísticas básicas del dataset\n",
    "    numeric_cols = df_anomalias.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        # Tomar las primeras 10 columnas para no saturar el gráfico\n",
    "        cols_to_plot = numeric_cols[:10]\n",
    "        means = [df_anomalias[col].mean() for col in cols_to_plot]\n",
    "        stds = [df_anomalias[col].std() for col in cols_to_plot]\n",
    "        \n",
    "        x = np.arange(len(cols_to_plot))\n",
    "        width = 0.35\n",
    "        \n",
    "        # Normalizar para visualización\n",
    "        max_mean = max(means) if means else 1\n",
    "        max_std = max(stds) if stds else 1\n",
    "        means_norm = [m/max_mean for m in means]\n",
    "        stds_norm = [s/max_std for s in stds]\n",
    "        \n",
    "        ax4.bar(x - width/2, means_norm, width, label='Media (norm)', alpha=0.7, color='blue')\n",
    "        ax4.bar(x + width/2, stds_norm, width, label='Desv. Estándar (norm)', alpha=0.7, color='orange')\n",
    "        \n",
    "        ax4.set_title('📈 Estadísticas de Características Principales')\n",
    "        ax4.set_ylabel('Valor Normalizado')\n",
    "        ax4.set_xticks(x)\n",
    "        ax4.set_xticklabels([col[:15] + '...' if len(col) > 15 else col for col in cols_to_plot], \n",
    "                            rotation=45, ha='right')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3, axis='y')\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No hay columnas numéricas', transform=ax4.transAxes, \n",
    "                ha='center', va='center', fontsize=14)\n",
    "        ax4.set_title('📈 Estadísticas - No Disponible')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Crear visualizaciones\n",
    "try:\n",
    "    visualizaciones_integradas(df_con_caracteristicas, resultados_ml, resultados_prophet, resultados_evaluacion)\n",
    "    print(\"✅ Visualizaciones generadas exitosamente\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error en visualizaciones: {e}\")\n",
    "    print(\"Creando visualización básica...\")\n",
    "    \n",
    "    # Visualización básica de respaldo\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    if 'anomalia' in df_con_caracteristicas.columns:\n",
    "        anomalias_count = df_con_caracteristicas['anomalia'].value_counts()\n",
    "        plt.bar(['Normal', 'Anomalía'], [anomalias_count.get(0, 0), anomalias_count.get(1, 0)], \n",
    "               color=['green', 'red'], alpha=0.7)\n",
    "        plt.title('📊 Distribución de Anomalías Detectadas')\n",
    "        plt.ylabel('Cantidad')\n",
    "        \n",
    "        # Agregar valores en las barras\n",
    "        for i, v in enumerate([anomalias_count.get(0, 0), anomalias_count.get(1, 0)]):\n",
    "            plt.text(i, v + max(anomalias_count.values) * 0.01, str(v), ha='center', va='bottom')\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No hay datos de anomalías disponibles', transform=plt.gca().transAxes, \n",
    "                ha='center', va='center', fontsize=14)\n",
    "        plt.title('📊 Estado del Sistema')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELDA 9: REPORTE INTEGRADO FINAL\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n📋 GENERANDO REPORTE INTEGRADO FINAL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def generar_reporte_integrado(df_caracteristicas, resultados_ml, resultados_prophet, resultados_eval):\n",
    "    \"\"\"Generar reporte que combina todos los análisis\"\"\"\n",
    "    \n",
    "    reporte = {\n",
    "        'timestamp': pd.Timestamp.now(),\n",
    "        'sistema': {\n",
    "            'modulos_integrados': modulos_disponibles,\n",
    "            'prophet_disponible': prophet_disponible,\n",
    "            'caracteristicas_avanzadas': caracteristicas_creadas,\n",
    "            'ml_ejecutado': ml_exitoso if 'ml_exitoso' in locals() else False\n",
    "        },\n",
    "        'datos': {\n",
    "            'registros_totales': len(df_caracteristicas),\n",
    "            'caracteristicas_totales': len(df_caracteristicas.columns),\n",
    "            'anomalias_detectadas': df_caracteristicas.get('anomalia', pd.Series([0])).sum(),\n",
    "            'periodo_analizado': {\n",
    "                'inicio': df_caracteristicas.index.min() if hasattr(df_caracteristicas.index, 'min') else 'N/A',\n",
    "                'fin': df_caracteristicas.index.max() if hasattr(df_caracteristicas.index, 'max') else 'N/A'\n",
    "            }\n",
    "        },\n",
    "        'ml_results': {},\n",
    "        'prophet_results': {},\n",
    "        'conclusiones': []\n",
    "    }\n",
    "    \n",
    "    # Resultados de ML\n",
    "    if resultados_eval and len(resultados_eval) > 0:\n",
    "        mejor_modelo = max(resultados_eval.keys(), \n",
    "                          key=lambda x: resultados_eval[x]['f1'])\n",
    "        reporte['ml_results'] = {\n",
    "            'mejor_modelo': mejor_modelo,\n",
    "            'mejor_f1_score': resultados_eval[mejor_modelo]['f1'],\n",
    "            'modelos_evaluados': len(resultados_eval),\n",
    "            'todos_los_modelos': {\n",
    "                modelo: {\n",
    "                    'f1': res['f1'],\n",
    "                    'precision': res['precision'],\n",
    "                    'recall': res['recall']\n",
    "                } for modelo, res in resultados_eval.items()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        reporte['conclusiones'].append(\n",
    "            f\"✅ Mejor modelo ML: {mejor_modelo} (F1: {resultados_eval[mejor_modelo]['f1']:.3f})\"\n",
    "        )\n",
    "    else:\n",
    "        reporte['conclusiones'].append(\"⚠️ Modelos ML no evaluados\")\n",
    "    \n",
    "    # Resultados de Prophet\n",
    "    if resultados_prophet:\n",
    "        reporte['prophet_results'] = {\n",
    "            'mae': resultados_prophet['metrics']['mae'],\n",
    "            'rmse': resultados_prophet['metrics']['rmse'],\n",
    "            'smape': resultados_prophet['metrics']['smape'],\n",
    "            'predicciones_futuras': len(resultados_prophet['forecast']) - len(resultados_prophet['data_used']),\n",
    "            'datos_entrenamiento': len(resultados_prophet['data_used'])\n",
    "        }\n",
    "        \n",
    "        calidad_prophet = \"Excelente\" if resultados_prophet['metrics']['smape'] < 10 else \\\n",
    "                         \"Buena\" if resultados_prophet['metrics']['smape'] < 20 else \"Regular\"\n",
    "        \n",
    "        reporte['conclusiones'].append(\n",
    "            f\"✅ Prophet SMAPE: {resultados_prophet['metrics']['smape']:.1f}% ({calidad_prophet})\"\n",
    "        )\n",
    "    else:\n",
    "        reporte['conclusiones'].append(\"⚠️ Análisis Prophet no completado\")\n",
    "    \n",
    "    # Análisis de anomalías\n",
    "    if 'anomalia' in df_caracteristicas.columns:\n",
    "        anomalias = df_caracteristicas['anomalia']\n",
    "        total_anomalias = anomalias.sum()\n",
    "        porcentaje_anomalias = (total_anomalias / len(anomalias)) * 100\n",
    "        \n",
    "        reporte['conclusiones'].extend([\n",
    "            f\"📊 Anomalías detectadas: {total_anomalias} ({porcentaje_anomalias:.2f}%)\",\n",
    "        ])\n",
    "        \n",
    "        if porcentaje_anomalias < 1:\n",
    "            reporte['conclusiones'].append(\"✅ Sistema operando dentro de parámetros normales\")\n",
    "        elif porcentaje_anomalias < 5:\n",
    "            reporte['conclusiones'].append(\"⚠️ Algunas anomalías detectadas - Monitoreo recomendado\")\n",
    "        else:\n",
    "            reporte['conclusiones'].append(\"🚨 Alto número de anomalías - Intervención requerida\")\n",
    "    \n",
    "    # Estado del sistema integrado\n",
    "    if modulos_disponibles and caracteristicas_creadas:\n",
    "        reporte['conclusiones'].append(\"🚀 Sistema integrado funcionando correctamente\")\n",
    "        reporte['conclusiones'].append(\"✅ Listo para despliegue en producción\")\n",
    "    else:\n",
    "        reporte['conclusiones'].append(\"⚠️ Sistema funcionando en modo básico\")\n",
    "    \n",
    "    return reporte\n",
    "\n",
    "# Generar reporte final\n",
    "reporte_final = generar_reporte_integrado(\n",
    "    df_con_caracteristicas, \n",
    "    resultados_ml, \n",
    "    resultados_prophet, \n",
    "    resultados_evaluacion if 'resultados_evaluacion' in locals() else None\n",
    ")\n",
    "\n",
    "# Mostrar reporte\n",
    "print(\"🏆 REPORTE INTEGRADO COMPLETADO\")\n",
    "print(\"=\"*40)\n",
    "print(f\"📅 Generado: {reporte_final['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\n🔧 ESTADO DEL SISTEMA:\")\n",
    "print(f\"   - Módulos integrados: {'✅' if reporte_final['sistema']['modulos_integrados'] else '❌'}\")\n",
    "print(f\"   - Prophet disponible: {'✅' if reporte_final['sistema']['prophet_disponible'] else '❌'}\")\n",
    "print(f\"   - Características avanzadas: {'✅' if reporte_final['sistema']['caracteristicas_avanzadas'] else '❌'}\")\n",
    "print(f\"   - ML ejecutado: {'✅' if reporte_final['sistema']['ml_ejecutado'] else '❌'}\")\n",
    "\n",
    "print(f\"\\n📊 DATOS PROCESADOS:\")\n",
    "print(f\"   - Registros: {reporte_final['datos']['registros_totales']:,}\")\n",
    "print(f\"   - Características: {reporte_final['datos']['caracteristicas_totales']}\")\n",
    "print(f\"   - Anomalías: {reporte_final['datos']['anomalias_detectadas']}\")\n",
    "\n",
    "if reporte_final['ml_results']:\n",
    "    print(f\"\\n🤖 RESULTADOS ML:\")\n",
    "    print(f\"   - Mejor modelo: {reporte_final['ml_results']['mejor_modelo']}\")\n",
    "    print(f\"   - F1-Score: {reporte_final['ml_results']['mejor_f1_score']:.3f}\")\n",
    "    print(f\"   - Modelos evaluados: {reporte_final['ml_results']['modelos_evaluados']}\")\n",
    "\n",
    "if reporte_final['prophet_results']:\n",
    "    print(f\"\\n🔮 RESULTADOS PROPHET:\")\n",
    "    print(f\"   - SMAPE: {reporte_final['prophet_results']['smape']:.2f}%\")\n",
    "    print(f\"   - MAE: {reporte_final['prophet_results']['mae']:.2f}\")\n",
    "    print(f\"   - Predicciones futuras: {reporte_final['prophet_results']['predicciones_futuras']}\")\n",
    "\n",
    "print(f\"\\n💡 CONCLUSIONES:\")\n",
    "for i, conclusion in enumerate(reporte_final['conclusiones'], 1):\n",
    "    print(f\"   {i}. {conclusion}\")\n",
    "\n",
    "# Calcular score general del sistema\n",
    "score_sistema = 0\n",
    "if reporte_final['sistema']['modulos_integrados']: score_sistema += 25\n",
    "if reporte_final['sistema']['prophet_disponible']: score_sistema += 25\n",
    "if reporte_final['sistema']['caracteristicas_avanzadas']: score_sistema += 25\n",
    "if reporte_final['sistema']['ml_ejecutado']: score_sistema += 25\n",
    "\n",
    "print(f\"\\n🎯 SCORE GENERAL DEL SISTEMA: {score_sistema}/100\")\n",
    "if score_sistema >= 75:\n",
    "    print(\"   Estado: 🟢 EXCELENTE - Sistema completamente funcional\")\n",
    "elif score_sistema >= 50:\n",
    "    print(\"   Estado: 🟡 BUENO - Sistema funcional con limitaciones\")\n",
    "else:\n",
    "    print(\"   Estado: 🔴 BÁSICO - Sistema funcionando en modo limitado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELDA 10: GUARDAR RESULTADOS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n💾 GUARDANDO RESULTADOS INTEGRADOS\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "archivos_guardados = []\n",
    "\n",
    "try:\n",
    "    # Guardar dataset con características\n",
    "    archivo_principal = 'anomalias_detectadas_INTEGRADO.csv'\n",
    "    df_con_caracteristicas.to_csv(archivo_principal)\n",
    "    archivos_guardados.append(archivo_principal)\n",
    "    print(f\"✓ Dataset principal guardado: {archivo_principal}\")\n",
    "    print(f\"   - Registros: {len(df_con_caracteristicas):,}\")\n",
    "    print(f\"   - Columnas: {len(df_con_caracteristicas.columns)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error guardando dataset principal: {e}\")\n",
    "\n",
    "try:\n",
    "    # Guardar reporte en JSON\n",
    "    import json\n",
    "    \n",
    "    # Convertir timestamps para JSON\n",
    "    reporte_json = reporte_final.copy()\n",
    "    reporte_json['timestamp'] = reporte_final['timestamp'].isoformat()\n",
    "    \n",
    "    if isinstance(reporte_json['datos']['periodo_analizado']['inicio'], pd.Timestamp):\n",
    "        reporte_json['datos']['periodo_analizado']['inicio'] = reporte_final['datos']['periodo_analizado']['inicio'].isoformat()\n",
    "    if isinstance(reporte_json['datos']['periodo_analizado']['fin'], pd.Timestamp):\n",
    "        reporte_json['datos']['periodo_analizado']['fin'] = reporte_final['datos']['periodo_analizado']['fin'].isoformat()\n",
    "    \n",
    "    archivo_reporte = 'reporte_integrado.json'\n",
    "    with open(archivo_reporte, 'w', encoding='utf-8') as f:\n",
    "        json.dump(reporte_json, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    archivos_guardados.append(archivo_reporte)\n",
    "    print(f\"✓ Reporte JSON guardado: {archivo_reporte}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error guardando reporte JSON: {e}\")\n",
    "\n",
    "try:\n",
    "    # Guardar predicciones Prophet si existen\n",
    "    if resultados_prophet:\n",
    "        archivo_prophet = 'predicciones_prophet_integrado.csv'\n",
    "        resultados_prophet['forecast'].to_csv(archivo_prophet, index=False)\n",
    "        archivos_guardados.append(archivo_prophet)\n",
    "        print(f\"✓ Predicciones Prophet guardadas: {archivo_prophet}\")\n",
    "        print(f\"   - Predicciones: {len(resultados_prophet['forecast'])}\")\n",
    "        print(f\"   - SMAPE: {resultados_prophet['metrics']['smape']:.2f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error guardando predicciones Prophet: {e}\")\n",
    "\n",
    "try:\n",
    "    # Guardar resultados ML si existen\n",
    "    if 'resultados_evaluacion' in locals() and resultados_evaluacion:\n",
    "        # Crear DataFrame con resultados ML\n",
    "        ml_results_df = pd.DataFrame({\n",
    "            'modelo': list(resultados_evaluacion.keys()),\n",
    "            'f1_score': [res['f1'] for res in resultados_evaluacion.values()],\n",
    "            'precision': [res['precision'] for res in resultados_evaluacion.values()],\n",
    "            'recall': [res['recall'] for res in resultados_evaluacion.values()],\n",
    "            'accuracy': [res['accuracy'] for res in resultados_evaluacion.values()]\n",
    "        })\n",
    "        \n",
    "        archivo_ml = 'resultados_ml_integrado.csv'\n",
    "        ml_results_df.to_csv(archivo_ml, index=False)\n",
    "        archivos_guardados.append(archivo_ml)\n",
    "        print(f\"✓ Resultados ML guardados: {archivo_ml}\")\n",
    "        print(f\"   - Modelos evaluados: {len(ml_results_df)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error guardando resultados ML: {e}\")\n",
    "\n",
    "# Crear archivo de resumen\n",
    "try:\n",
    "    archivo_resumen = 'RESUMEN_EJECUCION.txt'\n",
    "    with open(archivo_resumen, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"🔋 RESUMEN DE EJECUCIÓN - SISTEMA INTEGRADO\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        f.write(f\"Fecha de ejecución: {reporte_final['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Score del sistema: {score_sistema}/100\\n\\n\")\n",
    "        \n",
    "        f.write(\"📁 ARCHIVOS GENERADOS:\\n\")\n",
    "        for i, archivo in enumerate(archivos_guardados, 1):\n",
    "            f.write(f\"   {i}. {archivo}\\n\")\n",
    "        \n",
    "        f.write(\"\\n💡 CONCLUSIONES:\\n\")\n",
    "        for i, conclusion in enumerate(reporte_final['conclusiones'], 1):\n",
    "            f.write(f\"   {i}. {conclusion}\\n\")\n",
    "        \n",
    "        f.write(\"\\n🔧 CONFIGURACIÓN:\\n\")\n",
    "        f.write(f\"   - Módulos integrados: {reporte_final['sistema']['modulos_integrados']}\\n\")\n",
    "        f.write(f\"   - Prophet disponible: {reporte_final['sistema']['prophet_disponible']}\\n\")\n",
    "        f.write(f\"   - Características avanzadas: {reporte_final['sistema']['caracteristicas_avanzadas']}\\n\")\n",
    "        f.write(f\"   - ML ejecutado: {reporte_final['sistema']['ml_ejecutado']}\\n\")\n",
    "    \n",
    "    archivos_guardados.append(archivo_resumen)\n",
    "    print(f\"✓ Resumen guardado: {archivo_resumen}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error guardando resumen: {e}\")\n",
    "\n",
    "print(f\"\\n📂 ARCHIVOS GENERADOS TOTAL: {len(archivos_guardados)}\")\n",
    "for i, archivo in enumerate(archivos_guardados, 1):\n",
    "    print(f\"   {i}. {archivo}\")\n",
    "\n",
    "print(f\"\\n✅ Proceso de guardado completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CELDA 11: SISTEMA DE ALERTAS FINAL\n",
    "# ========================================\n",
    "\n",
    "def sistema_alertas_integrado(df_anomalias, resultados_prophet, umbral_critico=0.95):\n",
    "    \"\"\"Sistema de alertas que integra todos los métodos\"\"\"\n",
    "    print(\"\\n🚨 SISTEMA DE ALERTAS INTEGRADO\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    alertas_activas = []\n",
    "    \n",
    "    # 1. Verificar anomalías recientes\n",
    "    if 'anomalia' in df_anomalias.columns:\n",
    "        total_anomalias = df_anomalias['anomalia'].sum()\n",
    "        porcentaje_anomalias = (total_anomalias / len(df_anomalias)) * 100\n",
    "        \n",
    "        # Alerta por alta frecuencia de anomalías\n",
    "        if porcentaje_anomalias > 10:\n",
    "            alertas_activas.append({\n",
    "                'tipo': 'ALTA_FRECUENCIA_ANOMALIAS',\n",
    "                'severidad': 'CRÍTICA',\n",
    "                'mensaje': f'{total_anomalias} anomalías detectadas ({porcentaje_anomalias:.1f}%)',\n",
    "                'accion_recomendada': 'Inspección técnica inmediata del sistema'\n",
    "            })\n",
    "        elif porcentaje_anomalias > 5:\n",
    "            alertas_activas.append({\n",
    "                'tipo': 'FRECUENCIA_ANOMALIAS_ELEVADA',\n",
    "                'severidad': 'ALTA',\n",
    "                'mensaje': f'{total_anomalias} anomalías detectadas ({porcentaje_anomalias:.1f}%)',\n",
    "                'accion_recomendada': 'Revisión técnica programada'\n",
    "            })\n",
    "        elif porcentaje_anomalias > 1:\n",
    "            alertas_activas.append({\n",
    "                'tipo': 'ANOMALIAS_DETECTADAS',\n",
    "                'severidad': 'MEDIA',\n",
    "                'mensaje': f'{total_anomalias} anomalías detectadas ({porcentaje_anomalias:.1f}%)',\n",
    "                'accion_recomendada': 'Monitoreo continuo recomendado'\n",
    "            })\n",
    "    \n",
    "    # 2. Verificar tendencias Prophet (si disponible)\n",
    "    if resultados_prophet:\n",
    "        forecast = resultados_prophet['forecast']\n",
    "        \n",
    "        # Análisis de tendencia en predicciones\n",
    "        if len(forecast) > 30:\n",
    "            tendencia_reciente = forecast['trend'].tail(30).pct_change().mean()\n",
    "            \n",
    "            if tendencia_reciente < -0.10:  # Tendencia negativa > 10%\n",
    "                alertas_activas.append({\n",
    "                    'tipo': 'TENDENCIA_NEGATIVA_CRITICA',\n",
    "                    'severidad': 'ALTA',\n",
    "                    'mensaje': f'Tendencia de producción muy negativa: {tendencia_reciente:.2%}',\n",
    "                    'accion_recomendada': 'Revisión urgente del sistema y mantenimiento'\n",
    "                })\n",
    "            elif tendencia_reciente < -0.05:  # Tendencia negativa > 5%\n",
    "                alertas_activas.append({\n",
    "                    'tipo': 'TENDENCIA_NEGATIVA',\n",
    "                    'severidad': 'MEDIA',\n",
    "                    'mensaje': f'Tendencia de producción negativa: {tendencia_reciente:.2%}',\n",
    "                    'accion_recomendada': 'Revisión de mantenimiento preventivo'\n",
    "                })\n",
    "        \n",
    "        # Verificar calidad del modelo Prophet\n",
    "        smape = resultados_prophet['metrics']['smape']\n",
    "        if smape > 25:\n",
    "            alertas_activas.append({\n",
    "                'tipo': 'MODELO_PREDICTIVO_DEGRADADO',\n",
    "                'severidad': 'MEDIA',\n",
    "                'mensaje': f'Calidad del modelo predictivo baja (SMAPE: {smape:.1f}%)',\n",
    "                'accion_recomendada': 'Reentrenamiento del modelo recomendado'\n",
    "            })\n",
    "    \n",
    "    # 3. Verificar estado de integración del sistema\n",
    "    if not modulos_disponibles:\n",
    "        alertas_activas.append({\n",
    "            'tipo': 'SISTEMA_MODO_BASICO',\n",
    "            'severidad': 'BAJA',\n",
    "            'mensaje': 'Sistema funcionando en modo básico (módulos no disponibles)',\n",
    "            'accion_recomendada': 'Verificar instalación de módulos propios'\n",
    "        })\n",
    "    \n",
    "    if not prophet_disponible:\n",
    "        alertas_activas.append({\n",
    "            'tipo': 'PREDICCIONES_NO_DISPONIBLES',\n",
    "            'severidad': 'BAJA',\n",
    "            'mensaje': 'Análisis predictivo no disponible (Prophet no instalado)',\n",
    "            'accion_recomendada': 'Instalar Prophet para análisis predictivo completo'\n",
    "        })\n",
    "    \n",
    "    # 4. Mostrar alertas\n",
    "    if alertas_activas:\n",
    "        print(f\"⚠️ {len(alertas_activas)} ALERTAS ACTIVAS:\")\n",
    "        print()\n",
    "        \n",
    "        # Agrupar por severidad\n",
    "        alertas_por_severidad = {}\n",
    "        for alerta in alertas_activas:\n",
    "            sev = alerta['severidad']\n",
    "            if sev not in alertas_por_severidad:\n",
    "                alertas_por_severidad[sev] = []\n",
    "            alertas_por_severidad[sev].append(alerta)\n",
    "        \n",
    "        # Mostrar por orden de severidad\n",
    "        orden_severidad = ['CRÍTICA', 'ALTA', 'MEDIA', 'BAJA']\n",
    "        for severidad in orden_severidad:\n",
    "            if severidad in alertas_por_severidad:\n",
    "                icon = {'CRÍTICA': '🔴', 'ALTA': '🟠', 'MEDIA': '🟡', 'BAJA': '🔵'}[severidad]\n",
    "                print(f\"{icon} SEVERIDAD {severidad}:\")\n",
    "                \n",
    "                for i, alerta in enumerate(alertas_por_severidad[severidad], 1):\n",
    "                    print(f\"   {i}. {alerta['tipo']}\")\n",
    "                    print(f\"      {alerta['mensaje']}\")\n",
    "                    print(f\"      Acción: {alerta['accion_recomendada']}\")\n",
    "                    print()\n",
    "    else:\n",
    "        print(\"✅ No hay alertas activas\")\n",
    "        print(\"   Sistema operando normalmente\")\n",
    "    \n",
    "    # 5. Recomendaciones generales\n",
    "    print(\"\\n💡 RECOMENDACIONES GENERALES:\")\n",
    "    \n",
    "    if score_sistema >= 75:\n",
    "        print(\"   ✅ Sistema funcionando óptimamente\")\n",
    "        print(\"   ✅ Listo para despliegue en producción\")\n",
    "        print(\"   ✅ Monitoreo automático recomendado\")\n",
    "    elif score_sistema >= 50:\n",
    "        print(\"   🟡 Sistema funcional con mejoras posibles\")\n",
    "        print(\"   🟡 Considerar actualización de módulos\")\n",
    "        print(\"   🟡 Monitoreo manual recomendado\")\n",
    "    else:\n",
    "        print(\"   🔴 Sistema necesita mejoras\")\n",
    "        print(\"   🔴 Instalar módulos faltantes\")\n",
    "        print(\"   🔴 Supervisión técnica requerida\")\n",
    "    \n",
    "    return alertas_activas\n",
    "\n",
    "# Ejecutar sistema de alertas\n",
    "alertas = sistema_alertas_integrado(df_con_caracteristicas, resultados_prophet)\n",
    "\n",
    "print(f\"\\n🏁 SISTEMA INTEGRADO COMPLETADO\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"✅ Notebook ejecutado: 11 celdas\")\n",
    "print(f\"✅ Módulos integrados: {'Sí' if modulos_disponibles else 'No'}\")\n",
    "print(f\"✅ Análisis ML: {'Sí' if 'ml_exitoso' in locals() and ml_exitoso else 'No'}\")\n",
    "print(f\"✅ Análisis Prophet: {'Sí' if resultados_prophet else 'No'}\")\n",
    "print(f\"✅ Sistema de alertas: {len(alertas)} alertas activas\")\n",
    "print(f\"✅ Archivos guardados: {len(archivos_guardados)}\")\n",
    "print(f\"\\n🎯 Score final del sistema: {score_sistema}/100\")\n",
    "print(f\"\\n🚀 ¡Sistema integrado listo para usar!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
