{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy import stats\n",
    "\n",
    "# ==========================================\n",
    "# 1. FUNCI√ìN PARA BUSCAR ARCHIVOS CSV\n",
    "# ==========================================\n",
    "\n",
    "def find_csv_files(directorio_raiz='.'):\n",
    "    directorios_con_csv = []\n",
    "    for raiz, directorios, archivos in os.walk(directorio_raiz):\n",
    "        archivos_csv = [archivo for archivo in archivos if archivo.lower().endswith('.csv')]\n",
    "        if archivos_csv:\n",
    "            directorios_con_csv.append(raiz)\n",
    "    return directorios_con_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 2. CARGAR Y COMBINAR LOS 3 DATASETS\n",
    "# ==========================================\n",
    "\n",
    "def cargar_datasets():\n",
    "    \"\"\"\n",
    "    Funci√≥n para cargar los 3 datasets de paneles fotovoltaicos\n",
    "    \"\"\"\n",
    "    directorios = find_csv_files()\n",
    "    print(\"Directorios que contienen archivos CSV:\")\n",
    "    for directorio in directorios:\n",
    "        print(f\"- {directorio}\")\n",
    "    \n",
    "    # Buscar los archivos espec√≠ficos\n",
    "    environment_data = None\n",
    "    irradiance_data = None\n",
    "    electrical_data = None\n",
    "    \n",
    "    for directorio in directorios:\n",
    "        # Environment data\n",
    "        ruta_env = os.path.join(directorio, 'environment_data.csv')\n",
    "        if os.path.exists(ruta_env):\n",
    "            environment_data = pd.read_csv(ruta_env)\n",
    "            print(f\"‚úì Cargado: {ruta_env}\")\n",
    "        \n",
    "        # Irradiance data\n",
    "        ruta_irr = os.path.join(directorio, 'irradiance_data.csv')\n",
    "        if os.path.exists(ruta_irr):\n",
    "            irradiance_data = pd.read_csv(ruta_irr)\n",
    "            print(f\"‚úì Cargado: {ruta_irr}\")\n",
    "        \n",
    "        # Electrical data (inversores)\n",
    "        ruta_elec = os.path.join(directorio, 'electrical_data.csv')\n",
    "        if os.path.exists(ruta_elec):\n",
    "            electrical_data = pd.read_csv(ruta_elec)\n",
    "            print(f\"‚úì Cargado: {ruta_elec}\")\n",
    "    \n",
    "    return environment_data, irradiance_data, electrical_data\n",
    "\n",
    "def seleccionar_variables_inversor_1(electrical_data):\n",
    "    \"\"\"\n",
    "    Seleccionar solo las variables del inversor 1 y limpiar nombres\n",
    "    \"\"\"\n",
    "    # Seleccionar columnas del inversor 1\n",
    "    columnas_inv1 = ['measured_on']\n",
    "    for col in electrical_data.columns:\n",
    "        if 'inv_01_' in col:\n",
    "            columnas_inv1.append(col)\n",
    "    \n",
    "    df_inv1 = electrical_data[columnas_inv1].copy()\n",
    "    \n",
    "    # Limpiar nombres de columnas (quitar el sufijo _inv_XXXXX)\n",
    "    columnas_limpias = {}\n",
    "    for columna in df_inv1.columns:\n",
    "        if columna == 'measured_on':\n",
    "            columnas_limpias[columna] = columna\n",
    "        elif columna.startswith('inv_01_'):\n",
    "            # Extraer solo la parte del tipo de medici√≥n\n",
    "            partes = columna.split('_inv_')\n",
    "            if len(partes) >= 1:\n",
    "                nuevo_nombre = partes[0].replace('inv_01_', '')\n",
    "                columnas_limpias[columna] = nuevo_nombre\n",
    "    \n",
    "    df_inv1 = df_inv1.rename(columns=columnas_limpias)\n",
    "    return df_inv1\n",
    "\n",
    "def combinar_datasets(environment_data, irradiance_data, electrical_data):\n",
    "    \"\"\"\n",
    "    Realizar INNER JOIN de los 3 datasets por measured_on\n",
    "    \"\"\"\n",
    "    # Seleccionar variables del inversor 1\n",
    "    df_inv1 = seleccionar_variables_inversor_1(electrical_data)\n",
    "    \n",
    "    print(\"Variables seleccionadas del inversor 1:\")\n",
    "    print(df_inv1.columns.tolist())\n",
    "    \n",
    "    # Convertir measured_on a datetime en todos los datasets\n",
    "    environment_data['measured_on'] = pd.to_datetime(environment_data['measured_on'])\n",
    "    irradiance_data['measured_on'] = pd.to_datetime(irradiance_data['measured_on'])\n",
    "    df_inv1['measured_on'] = pd.to_datetime(df_inv1['measured_on'])\n",
    "    \n",
    "    # Realizar INNER JOIN\n",
    "    print(\"\\nüìä Realizando INNER JOIN...\")\n",
    "    df_combined = environment_data.merge(irradiance_data, on='measured_on', how='inner')\n",
    "    df_final = df_combined.merge(df_inv1, on='measured_on', how='inner')\n",
    "    \n",
    "    print(f\"‚úì Dataset final: {len(df_final)} filas, {len(df_final.columns)} columnas\")\n",
    "    print(\"Columnas finales:\", df_final.columns.tolist())\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. PREPROCESAMIENTO Y PROMEDIOS HORARIOS\n",
    "# ==========================================\n",
    "\n",
    "def crear_promedios_horarios(df, metodo_agregacion='mean', incluir_estadisticas=True):\n",
    "    \"\"\"\n",
    "        Crear promedios horarios con opciones mejoradas de suavizado\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : DataFrame\n",
    "            DataFrame con timestamp en 'measured_on'\n",
    "        metodo_agregacion : str\n",
    "            'mean', 'median' o 'weighted_mean'\n",
    "        incluir_estadisticas : bool\n",
    "        Si incluir estad√≠sticas adicionales (std, min, max)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üìä CREANDO PROMEDIOS HORARIOS MEJORADOS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    df = df.copy()\n",
    "    df = df.set_index('measured_on')\n",
    "        # Fin de la selecci√≥n\n",
    "    \n",
    "    print(f\"‚úì Datos originales: {len(df)} registros\")\n",
    "    print(f\"‚úì Frecuencia original: {pd.infer_freq(df.index) or 'Variable'}\")\n",
    "    print(f\"‚úì Per√≠odo: {df.index.min()} a {df.index.max()}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 1. DIFERENTES M√âTODOS DE AGREGACI√ìN\n",
    "    # ========================================\n",
    "    \n",
    "    if metodo_agregacion == 'mean':\n",
    "        # Promedio simple (tu m√©todo actual)\n",
    "        df_hourly = df.groupby(df.index.floor('H')).mean()\n",
    "        print(\"‚úì Usando: Promedio aritm√©tico\")\n",
    "        \n",
    "    elif metodo_agregacion == 'median':\n",
    "        # Mediana (m√°s robusta a outliers)\n",
    "        df_hourly = df.groupby(df.index.floor('H')).median()\n",
    "        print(\"‚úì Usando: Mediana (m√°s robusta)\")\n",
    "        \n",
    "    elif metodo_agregacion == 'weighted_mean':\n",
    "        # Promedio ponderado (m√°s peso a datos recientes en la hora)\n",
    "        df_hourly = _promedio_ponderado_horario(df)\n",
    "        print(\"‚úì Usando: Promedio ponderado temporal\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 2. ESTAD√çSTICAS ADICIONALES (OPCIONAL)\n",
    "    # ========================================\n",
    "    \n",
    "    if incluir_estadisticas:\n",
    "        # Calcular desviaci√≥n est√°ndar horaria (√∫til para detectar variabilidad)\n",
    "        df_std = df.groupby(df.index.floor('H')).std()\n",
    "        df_min = df.groupby(df.index.floor('H')).min()\n",
    "        df_max = df.groupby(df.index.floor('H')).max()\n",
    "        df_count = df.groupby(df.index.floor('H')).count()\n",
    "        \n",
    "        # A√±adir columnas de estad√≠sticas\n",
    "        for col in df_hourly.columns:\n",
    "            if col in df_std.columns:\n",
    "                df_hourly[f'{col}_std'] = df_std[col]\n",
    "                df_hourly[f'{col}_min'] = df_min[col]\n",
    "                df_hourly[f'{col}_max'] = df_max[col]\n",
    "                df_hourly[f'{col}_count'] = df_count[col]\n",
    "    \n",
    "    print(f\"‚úì Datos horarios: {len(df_hourly)} registros\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 3. LIMPIEZA Y FILTRADO MEJORADO\n",
    "    # ========================================\n",
    "    \n",
    "    # Identificar horas con pocos datos (menos de la mitad de las observaciones esperadas)\n",
    "    if incluir_estadisticas:\n",
    "        observaciones_esperadas = 4  # 4 observaciones por hora (cada 15 min)\n",
    "        mask_pocos_datos = df_hourly[f'{df_hourly.columns[0]}_count'] < (observaciones_esperadas / 2)\n",
    "        \n",
    "        if mask_pocos_datos.sum() > 0:\n",
    "            print(f\"‚ö†Ô∏è {mask_pocos_datos.sum()} horas con pocos datos (< {observaciones_esperadas/2} obs)\")\n",
    "            # Opcional: marcar estas horas como sospechosas\n",
    "            df_hourly.loc[mask_pocos_datos, 'calidad_datos'] = 'baja'\n",
    "        else:\n",
    "            df_hourly['calidad_datos'] = 'alta'\n",
    "    \n",
    "    # Manejo inteligente de valores faltantes\n",
    "    df_hourly = _manejar_valores_faltantes_inteligente(df_hourly)\n",
    "    \n",
    "    # ========================================\n",
    "    # 4. FILTRADO POR OPERACI√ìN (MEJORADO)\n",
    "    # ========================================\n",
    "    \n",
    "    df_hourly_filtered = _filtrar_horas_operacion_mejorado(df_hourly)\n",
    "    \n",
    "    print(f\"‚úì Datos finales (solo operaci√≥n): {len(df_hourly_filtered)} registros\")\n",
    "    print(f\"   - Filtradas {len(df_hourly) - len(df_hourly_filtered)} horas sin generaci√≥n\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 5. VALIDACI√ìN DE CALIDAD FINAL\n",
    "    # ========================================\n",
    "    \n",
    "    _validar_calidad_suavizado(df, df_hourly_filtered)\n",
    "    \n",
    "    return df_hourly_filtered\n",
    "\n",
    "def _validar_calidad_suavizado(df_original, df_hourly):\n",
    "    \"\"\"\n",
    "    Validar la calidad del suavizado\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìà VALIDACI√ìN DE CALIDAD DEL SUAVIZADO\")\n",
    "    print(\"=\"*45)\n",
    "    \n",
    "    # Comparar estad√≠sticas b√°sicas\n",
    "    print(f\"‚úì Reducci√≥n de datos: {len(df_original)} ‚Üí {len(df_hourly)} \"\n",
    "          f\"({len(df_hourly)/len(df_original)*100:.1f}% retenido)\")\n",
    "    \n",
    "    # Comparar algunas variables clave\n",
    "    variables_clave = ['ac_power']\n",
    "    irradiance_col = [col for col in df_hourly.columns if 'irradiance' in col.lower() and '_std' not in col]\n",
    "    if irradiance_col:\n",
    "        variables_clave.append(irradiance_col[0])\n",
    "    \n",
    "    for var in variables_clave:\n",
    "        if var in df_original.columns and var in df_hourly.columns:\n",
    "            orig_mean = df_original[var].mean()\n",
    "            hourly_mean = df_hourly[var].mean()\n",
    "            diferencia_pct = abs(orig_mean - hourly_mean) / orig_mean * 100\n",
    "            \n",
    "            print(f\"   {var}:\")\n",
    "            print(f\"     Original: {orig_mean:.2f} | Horario: {hourly_mean:.2f} \"\n",
    "                  f\"(diferencia: {diferencia_pct:.1f}%)\")\n",
    "    \n",
    "    # Verificar continuidad temporal\n",
    "    gaps_temporales = df_hourly.index.to_series().diff().dt.total_seconds() / 3600\n",
    "    gaps_grandes = gaps_temporales[gaps_temporales > 1.5]  # Gaps > 1.5 horas\n",
    "    \n",
    "    if len(gaps_grandes) > 0:\n",
    "        print(f\"‚ö†Ô∏è {len(gaps_grandes)} gaps temporales > 1.5 horas detectados\")\n",
    "    else:\n",
    "        print(\"‚úì Continuidad temporal: OK\")\n",
    "\n",
    "def _promedio_ponderado_horario(df):\n",
    "    \"\"\"\n",
    "    Calcular promedio ponderado dando m√°s peso a datos m√°s recientes en cada hora\n",
    "    \"\"\"\n",
    "    df_hourly_list = []\n",
    "    \n",
    "    for hora, grupo in df.groupby(df.index.floor('H')):\n",
    "        if len(grupo) > 1:\n",
    "            # Crear pesos: m√°s peso a observaciones m√°s tard√≠as en la hora\n",
    "            pesos = np.linspace(0.5, 1.0, len(grupo))\n",
    "            pesos = pesos / pesos.sum()  # Normalizar\n",
    "            \n",
    "            # Promedio ponderado\n",
    "            resultado = {}\n",
    "            for col in grupo.select_dtypes(include=[np.number]).columns:\n",
    "                resultado[col] = np.average(grupo[col], weights=pesos)\n",
    "            \n",
    "            df_hourly_list.append(pd.Series(resultado, name=hora))\n",
    "        else:\n",
    "            # Si solo hay una observaci√≥n, usarla directamente\n",
    "            df_hourly_list.append(grupo.iloc[0])\n",
    "    \n",
    "    return pd.DataFrame(df_hourly_list)\n",
    "\n",
    "def _filtrar_horas_operacion_mejorado(df_hourly):\n",
    "    \"\"\"\n",
    "    Filtrado mejorado para horas de operaci√≥n\n",
    "    \"\"\"\n",
    "    print(f\"\\nüåû Filtrando horas de operaci√≥n...\")\n",
    "    \n",
    "    # Identificar columnas clave\n",
    "    irradiance_col = [col for col in df_hourly.columns if 'irradiance' in col.lower() and '_std' not in col][0]\n",
    "    power_col = 'ac_power'\n",
    "    \n",
    "    # Criterios m√∫ltiples para operaci√≥n (m√°s robustos)\n",
    "    criterios_operacion = [\n",
    "        df_hourly[power_col] > 10,  # Potencia m√≠nima\n",
    "        df_hourly[irradiance_col] > 50,  # Irradiancia m√≠nima\n",
    "        df_hourly.index.hour.isin(range(6, 19))  # Solo horas diurnas (6 AM - 7 PM)\n",
    "    ]\n",
    "    \n",
    "    # Combinar criterios (OR l√≥gico)\n",
    "    mask_operacion = criterios_operacion[0] | criterios_operacion[1]\n",
    "    mask_operacion = mask_operacion & criterios_operacion[2]  # AND con horas diurnas\n",
    "    \n",
    "    df_hourly_filtered = df_hourly[mask_operacion].copy()\n",
    "    \n",
    "    print(f\"   - Criterio 1 (Potencia > 10W): {criterios_operacion[0].sum()} horas\")\n",
    "    print(f\"   - Criterio 2 (Irradiancia > 50): {criterios_operacion[1].sum()} horas\")\n",
    "    print(f\"   - Criterio 3 (Horas diurnas): {criterios_operacion[2].sum()} horas\")\n",
    "    print(f\"   - Horas operativas finales: {len(df_hourly_filtered)}\")\n",
    "    \n",
    "    return df_hourly_filtered\n",
    "\n",
    "def _manejar_valores_faltantes_inteligente(df_hourly):\n",
    "    \"\"\"\n",
    "    Manejo inteligente de valores faltantes para datos horarios\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîß Manejando valores faltantes...\")\n",
    "    \n",
    "    valores_faltantes_antes = df_hourly.isnull().sum().sum()\n",
    "    \n",
    "    if valores_faltantes_antes > 0:\n",
    "        print(f\"   - Valores faltantes encontrados: {valores_faltantes_antes}\")\n",
    "        \n",
    "        # Para gaps peque√±os (‚â§ 3 horas): interpolaci√≥n lineal\n",
    "        df_hourly_interpolado = df_hourly.interpolate(method='linear', limit=3)\n",
    "        \n",
    "        # Para gaps m√°s grandes: interpolaci√≥n estacional (considera patrones diarios)\n",
    "        df_hourly_interpolado = df_hourly_interpolado.interpolate(method='time', limit=6)\n",
    "        \n",
    "        # Eliminar filas que a√∫n tienen NaN despu√©s de interpolaci√≥n\n",
    "        df_hourly_clean = df_hourly_interpolado.dropna()\n",
    "        \n",
    "        valores_faltantes_despues = df_hourly_clean.isnull().sum().sum()\n",
    "        print(f\"   - Valores faltantes despu√©s de limpieza: {valores_faltantes_despues}\")\n",
    "        print(f\"   - Filas eliminadas: {len(df_hourly) - len(df_hourly_clean)}\")\n",
    "        \n",
    "        return df_hourly_clean\n",
    "    else:\n",
    "        print(\"   - No se encontraron valores faltantes\")\n",
    "        return df_hourly\n",
    "\n",
    "# ========================================\n",
    "# FUNCI√ìN DE COMPARACI√ìN VISUAL\n",
    "# ========================================\n",
    "\n",
    "def comparar_suavizado_visual(df_original, df_hourly, variable='ac_power', dias_muestra=1000):\n",
    "    \"\"\"\n",
    "    Comparar visualmente el efecto del suavizado\n",
    "    \"\"\"\n",
    "    # Tomar una muestra de pocos d√≠as para ver el efecto\n",
    "    fecha_inicio = df_original.index.min()\n",
    "    fecha_fin = fecha_inicio + pd.Timedelta(days=dias_muestra)\n",
    "    \n",
    "    df_muestra = df_original[df_original.index <= fecha_fin]\n",
    "    df_hourly_muestra = df_hourly[df_hourly.index <= fecha_fin]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "    \n",
    "    # Gr√°fica 1: Datos originales (cada 15 min)\n",
    "    ax1.plot(df_muestra.index, df_muestra[variable], alpha=0.7, color='lightblue', \n",
    "             linewidth=0.8, label=f'{variable} (cada 15 min)')\n",
    "    ax1.set_title(f'Datos Originales - {variable} (Muestra de {dias_muestra} d√≠as)')\n",
    "    ax1.set_ylabel(variable)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fica 2: Datos suavizados (cada hora)\n",
    "    ax2.plot(df_hourly_muestra.index, df_hourly_muestra[variable], alpha=0.8, color='blue', \n",
    "             linewidth=2, marker='o', markersize=4, label=f'{variable} (horario)')\n",
    "    ax2.set_title(f'Datos Suavizados - {variable} (Promedios Horarios)')\n",
    "    ax2.set_ylabel(variable)\n",
    "    ax2.set_xlabel('Fecha y Hora')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Formatear fechas\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä COMPARACI√ìN DE SUAVIZADO ({dias_muestra} d√≠as):\")\n",
    "    print(f\"   - Puntos originales: {len(df_muestra)}\")\n",
    "    print(f\"   - Puntos suavizados: {len(df_hourly_muestra)}\")\n",
    "    print(f\"   - Factor de reducci√≥n: {len(df_muestra)/len(df_hourly_muestra):.1f}x\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. DETECTORES DE ANOMAL√çAS\n",
    "# ==========================================\n",
    "\n",
    "def detector_euclidean_distance(df, percentil=99):\n",
    "    \"\"\"\n",
    "    Detector de anomal√≠as con Distancia Euclidiana (1% m√°s lejanos para ser m√°s selectivo)\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç Detector 1: Distancia Euclidiana\")\n",
    "    \n",
    "    columnas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    datos = df[columnas_numericas].to_numpy()\n",
    "    \n",
    "    # Normalizar datos (muy importante para evitar que una variable domine)\n",
    "    scaler = StandardScaler()\n",
    "    datos_normalizados = scaler.fit_transform(datos)\n",
    "    \n",
    "    # Calcular distancias euclidianas al centroide\n",
    "    centroide = np.mean(datos_normalizados, axis=0)\n",
    "    distancias = np.sqrt(np.sum((datos_normalizados - centroide)**2, axis=1))\n",
    "    \n",
    "    # Umbral m√°s estricto: percentil 99 (1% m√°s lejano)\n",
    "    umbral = np.percentile(distancias, percentil)\n",
    "    anomalias_mask = distancias > umbral\n",
    "    \n",
    "    anomalias_euclidean = df.index[anomalias_mask]\n",
    "    \n",
    "    print(f\"   - Umbral: {umbral:.4f}\")\n",
    "    print(f\"   - Anomal√≠as detectadas: {len(anomalias_euclidean)} ({len(anomalias_euclidean)/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return anomalias_euclidean, distancias, umbral\n",
    "\n",
    "def detector_mahalanobis_distance(df, percentil=99):\n",
    "    \"\"\"\n",
    "    Detector de anomal√≠as con Distancia Mahalanobis (99% percentil)\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç Detector 2: Distancia Mahalanobis\")\n",
    "    \n",
    "    columnas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    datos = df[columnas_numericas].to_numpy()\n",
    "    \n",
    "    # Normalizar datos primero\n",
    "    scaler = StandardScaler()\n",
    "    datos_normalizados = scaler.fit_transform(datos)\n",
    "    \n",
    "    # Calcular matriz de covarianza y su inversa\n",
    "    cov_matrix = np.cov(datos_normalizados, rowvar=False)\n",
    "    \n",
    "    # Usar pseudoinversa para evitar problemas de singularidad\n",
    "    try:\n",
    "        inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "    except:\n",
    "        inv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "    \n",
    "    # Calcular distancias de Mahalanobis\n",
    "    centroide = np.mean(datos_normalizados, axis=0)\n",
    "    distancias = []\n",
    "    \n",
    "    for i in range(len(datos_normalizados)):\n",
    "        diff = datos_normalizados[i] - centroide\n",
    "        mahal_dist = np.sqrt(diff.T @ inv_cov_matrix @ diff)\n",
    "        distancias.append(mahal_dist)\n",
    "    \n",
    "    distancias = np.array(distancias)\n",
    "    \n",
    "    # Umbral para el percentil especificado\n",
    "    umbral = np.percentile(distancias, percentil)\n",
    "    anomalias_mask = distancias > umbral\n",
    "    \n",
    "    anomalias_mahalanobis = df.index[anomalias_mask]\n",
    "    \n",
    "    print(f\"   - Umbral: {umbral:.4f}\")\n",
    "    print(f\"   - Anomal√≠as detectadas: {len(anomalias_mahalanobis)} ({len(anomalias_mahalanobis)/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return anomalias_mahalanobis, distancias, umbral\n",
    "\n",
    "def detector_isolation_forest(df, contaminacion=0.01):\n",
    "    \"\"\"\n",
    "    Detector de anomal√≠as con Isolation Forest (1% contaminaci√≥n m√°s selectivo)\n",
    "    \"\"\"\n",
    "    print(\"\\nüîç Detector 3: Isolation Forest\")\n",
    "    \n",
    "    columnas_numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    datos = df[columnas_numericas]\n",
    "    \n",
    "    # Normalizar datos\n",
    "    scaler = StandardScaler()\n",
    "    datos_normalizados = scaler.fit_transform(datos)\n",
    "    \n",
    "    # Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=contaminacion, random_state=42, n_estimators=200)\n",
    "    anomalias_pred = iso_forest.fit_predict(datos_normalizados)\n",
    "    \n",
    "    # Obtener scores de anomal√≠a\n",
    "    anomaly_scores = iso_forest.decision_function(datos_normalizados)\n",
    "    \n",
    "    # Identificar anomal√≠as (predicci√≥n = -1)\n",
    "    anomalias_mask = anomalias_pred == -1\n",
    "    anomalias_isolation = df.index[anomalias_mask]\n",
    "    \n",
    "    print(f\"   - Contaminaci√≥n: {contaminacion*100}%\")\n",
    "    print(f\"   - Anomal√≠as detectadas: {len(anomalias_isolation)} ({len(anomalias_isolation)/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    return anomalias_isolation, anomaly_scores\n",
    "\n",
    "# ==========================================\n",
    "# 5. VISUALIZACI√ìN MEJORADA\n",
    "# ==========================================\n",
    "\n",
    "def plot_euclidean_distance_with_mask(df, anomalias_euclidean, distancias, umbral, \n",
    "                                      variable_principal='ac_voltage'):\n",
    "    \"\"\"\n",
    "    Graficar la distancia euclidiana con la m√°scara de anomal√≠as\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "    \n",
    "    # Verificar que la variable principal existe\n",
    "    if variable_principal not in df.columns:\n",
    "        posibles = [col for col in df.columns if 'voltage' in col.lower() or 'power' in col.lower()]\n",
    "        if posibles:\n",
    "            variable_principal = posibles[0]\n",
    "        else:\n",
    "            variable_principal = df.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    print(f\"üìä Graficando variable: {variable_principal}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # GR√ÅFICA 1: DISTANCIAS EUCLIDIANAS\n",
    "    # ========================================\n",
    "    \n",
    "    # Graficar todas las distancias\n",
    "    axes[0].plot(df.index, distancias, color='blue', alpha=0.7, linewidth=1, \n",
    "                label=f'Distancia Euclidiana')\n",
    "    \n",
    "    # L√≠nea del umbral\n",
    "    axes[0].axhline(y=umbral, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Umbral (percentil 99): {umbral:.4f}')\n",
    "    \n",
    "    # Resaltar anomal√≠as detectadas\n",
    "    mask_anomalias = distancias > umbral\n",
    "    anomalias_indices = np.where(mask_anomalias)[0]\n",
    "    \n",
    "    if len(anomalias_indices) > 0:\n",
    "        axes[0].scatter(df.index[anomalias_indices], distancias[anomalias_indices], \n",
    "                       color='red', s=50, alpha=0.8, zorder=5,\n",
    "                       label=f'Anomal√≠as detectadas ({len(anomalias_indices)})')\n",
    "    \n",
    "    # Rellenar √°rea de anomal√≠as\n",
    "    axes[0].fill_between(df.index, distancias, umbral, \n",
    "                        where=(distancias > umbral), \n",
    "                        color='red', alpha=0.2, interpolate=True,\n",
    "                        label='Zona an√≥mala')\n",
    "    \n",
    "    axes[0].set_title('Distancias Euclidianas y Detecci√≥n de Anomal√≠as')\n",
    "    axes[0].set_ylabel('Distancia Euclidiana')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ========================================\n",
    "    # GR√ÅFICA 2: VARIABLE PRINCIPAL CON ANOMAL√çAS\n",
    "    # ========================================\n",
    "    \n",
    "    # Graficar la variable principal\n",
    "    axes[1].plot(df.index, df[variable_principal], alpha=0.7, color='lightblue', \n",
    "                label=f'{variable_principal} (Normal)')\n",
    "    \n",
    "    # Superponer anomal√≠as detectadas\n",
    "    if len(anomalias_euclidean) > 0:\n",
    "        axes[1].scatter(anomalias_euclidean, df.loc[anomalias_euclidean, variable_principal], \n",
    "                       color='red', s=50, alpha=0.8, zorder=5,\n",
    "                       label=f'Anomal√≠as ({len(anomalias_euclidean)})')\n",
    "    \n",
    "    axes[1].set_title(f'Variable {variable_principal} con Anomal√≠as Detectadas')\n",
    "    axes[1].set_ylabel(variable_principal)\n",
    "    axes[1].set_xlabel('Fecha')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Formatear fechas en ambos ejes X\n",
    "    for ax in axes:\n",
    "        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # ========================================\n",
    "    # ESTAD√çSTICAS ADICIONALES\n",
    "    # ========================================\n",
    "    print(f\"\\nüìä ESTAD√çSTICAS DE DISTANCIA EUCLIDIANA:\")\n",
    "    print(f\"   - Distancia m√≠nima: {np.min(distancias):.4f}\")\n",
    "    print(f\"   - Distancia m√°xima: {np.max(distancias):.4f}\")\n",
    "    print(f\"   - Distancia promedio: {np.mean(distancias):.4f}\")\n",
    "    print(f\"   - Desviaci√≥n est√°ndar: {np.std(distancias):.4f}\")\n",
    "    print(f\"   - Umbral (percentil 99): {umbral:.4f}\")\n",
    "    print(f\"   - Anomal√≠as detectadas: {len(anomalias_euclidean)} ({len(anomalias_euclidean)/len(df)*100:.2f}%)\")\n",
    "\n",
    "def plot_anomalies_comparison_mejorado(df, anomalias_euclidean, anomalias_mahalanobis, anomalias_isolation,\n",
    "                                      dist_euclidean, dist_mahalanobis, scores_isolation,\n",
    "                                      umbral_euclidean, umbral_mahalanobis,\n",
    "                                      variable_principal='ac_voltage'):\n",
    "    \"\"\"\n",
    "    Versi√≥n mejorada que incluye las distancias para el m√©todo euclidiano\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(15, 16))\n",
    "    \n",
    "    if variable_principal not in df.columns:\n",
    "        posibles = [col for col in df.columns if 'voltage' in col.lower() or 'power' in col.lower()]\n",
    "        if posibles:\n",
    "            variable_principal = posibles[0]\n",
    "        else:\n",
    "            variable_principal = df.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    print(f\"üìä Graficando variable: {variable_principal}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # GR√ÅFICA 1: DISTANCIAS EUCLIDIANAS (NUEVA)\n",
    "    # ========================================\n",
    "    axes[0].plot(df.index, dist_euclidean, color='blue', alpha=0.7, linewidth=1, \n",
    "                label='Distancia Euclidiana')\n",
    "    axes[0].axhline(y=umbral_euclidean, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Umbral: {umbral_euclidean:.4f}')\n",
    "    \n",
    "    # Resaltar anomal√≠as\n",
    "    mask_anomalias = dist_euclidean > umbral_euclidean\n",
    "    anomalias_indices = np.where(mask_anomalias)[0]\n",
    "    \n",
    "    if len(anomalias_indices) > 0:\n",
    "        axes[0].scatter(df.index[anomalias_indices], dist_euclidean[anomalias_indices], \n",
    "                       color='red', s=30, alpha=0.8, zorder=5)\n",
    "        axes[0].fill_between(df.index, dist_euclidean, umbral_euclidean, \n",
    "                            where=(dist_euclidean > umbral_euclidean), \n",
    "                            color='red', alpha=0.2, interpolate=True)\n",
    "    \n",
    "    axes[0].set_title('Distancias Euclidianas y M√°scara de Anomal√≠as')\n",
    "    axes[0].set_ylabel('Distancia Euclidiana')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ========================================\n",
    "    # GR√ÅFICA 2: VARIABLE PRINCIPAL - EUCLIDIANA\n",
    "    # ========================================\n",
    "    axes[1].plot(df.index, df[variable_principal], alpha=0.7, color='lightblue', \n",
    "                label=variable_principal)\n",
    "    if len(anomalias_euclidean) > 0:\n",
    "        axes[1].scatter(anomalias_euclidean, df.loc[anomalias_euclidean, variable_principal], \n",
    "                       color='red', s=50, alpha=0.8, label=f'Anomal√≠as Euclidiana ({len(anomalias_euclidean)})')\n",
    "    axes[1].set_title('Detector 1: Distancia Euclidiana en Variable Principal')\n",
    "    axes[1].set_ylabel(variable_principal)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ========================================\n",
    "    # GR√ÅFICA 3: DISTANCIA MAHALANOBIS\n",
    "    # ========================================\n",
    "    axes[2].plot(df.index, df[variable_principal], alpha=0.7, color='lightblue', \n",
    "                label=variable_principal)\n",
    "    if len(anomalias_mahalanobis) > 0:\n",
    "        axes[2].scatter(anomalias_mahalanobis, df.loc[anomalias_mahalanobis, variable_principal], \n",
    "                       color='orange', s=50, alpha=0.8, label=f'Anomal√≠as Mahalanobis ({len(anomalias_mahalanobis)})')\n",
    "    axes[2].set_title('Detector 2: Distancia Mahalanobis')\n",
    "    axes[2].set_ylabel(variable_principal)\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ========================================\n",
    "    # GR√ÅFICA 4: ISOLATION FOREST\n",
    "    # ========================================\n",
    "    axes[3].plot(df.index, df[variable_principal], alpha=0.7, color='lightblue', \n",
    "                label=variable_principal)\n",
    "    if len(anomalias_isolation) > 0:\n",
    "        axes[3].scatter(anomalias_isolation, df.loc[anomalias_isolation, variable_principal], \n",
    "                       color='purple', s=50, alpha=0.8, label=f'Anomal√≠as Isolation Forest ({len(anomalias_isolation)})')\n",
    "    axes[3].set_title('Detector 3: Isolation Forest')\n",
    "    axes[3].set_ylabel(variable_principal)\n",
    "    axes[3].set_xlabel('Fecha')\n",
    "    axes[3].legend()\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Formatear fechas en todos los ejes X\n",
    "    for ax in axes:\n",
    "        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_anomalies_comparison(df, anomalias_euclidean, anomalias_mahalanobis, anomalias_isolation, \n",
    "                            variable_principal='ac_voltage'):\n",
    "    \"\"\"\n",
    "    Crear gr√°ficas comparativas de los 3 m√©todos de detecci√≥n (funci√≥n original)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "    \n",
    "    if variable_principal not in df.columns:\n",
    "        # Buscar una variable similar\n",
    "        posibles = [col for col in df.columns if 'voltage' in col.lower() or 'power' in col.lower()]\n",
    "        if posibles:\n",
    "            variable_principal = posibles[0]\n",
    "        else:\n",
    "            variable_principal = df.select_dtypes(include=[np.number]).columns[0]\n",
    "    \n",
    "    print(f\"üìä Graficando variable: {variable_principal}\")\n",
    "    \n",
    "    # Gr√°fica 1: Distancia Euclidiana\n",
    "    axes[0].plot(df.index, df[variable_principal], alpha=0.7, label=variable_principal)\n",
    "    if len(anomalias_euclidean) > 0:\n",
    "        axes[0].scatter(anomalias_euclidean, df.loc[anomalias_euclidean, variable_principal], \n",
    "                       color='red', s=50, alpha=0.8, label=f'Anomal√≠as Euclidiana ({len(anomalias_euclidean)})')\n",
    "    axes[0].set_title('Detector 1: Distancia Euclidiana')\n",
    "    axes[0].set_ylabel(variable_principal)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fica 2: Distancia Mahalanobis\n",
    "    axes[1].plot(df.index, df[variable_principal], alpha=0.7, label=variable_principal)\n",
    "    if len(anomalias_mahalanobis) > 0:\n",
    "        axes[1].scatter(anomalias_mahalanobis, df.loc[anomalias_mahalanobis, variable_principal], \n",
    "                       color='orange', s=50, alpha=0.8, label=f'Anomal√≠as Mahalanobis ({len(anomalias_mahalanobis)})')\n",
    "    axes[1].set_title('Detector 2: Distancia Mahalanobis')\n",
    "    axes[1].set_ylabel(variable_principal)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gr√°fica 3: Isolation Forest\n",
    "    axes[2].plot(df.index, df[variable_principal], alpha=0.7, label=variable_principal)\n",
    "    if len(anomalias_isolation) > 0:\n",
    "        axes[2].scatter(anomalias_isolation, df.loc[anomalias_isolation, variable_principal], \n",
    "                       color='purple', s=50, alpha=0.8, label=f'Anomal√≠as Isolation Forest ({len(anomalias_isolation)})')\n",
    "    axes[2].set_title('Detector 3: Isolation Forest')\n",
    "    axes[2].set_ylabel(variable_principal)\n",
    "    axes[2].set_xlabel('Fecha')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Formatear fechas en el eje X\n",
    "    for ax in axes:\n",
    "        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ==========================================\n",
    "# 6. AN√ÅLISIS Y COMPARACI√ìN\n",
    "# ==========================================\n",
    "\n",
    "def analizar_calidad_anomalias(df, anomalias_euclidean, anomalias_mahalanobis, anomalias_isolation):\n",
    "    \"\"\"\n",
    "    Analizar la calidad y caracter√≠sticas de las anomal√≠as detectadas\n",
    "    \"\"\"\n",
    "    print(\"\\nüî¨ AN√ÅLISIS DE CALIDAD DE ANOMAL√çAS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Estad√≠sticas de las variables en anomal√≠as vs normal\n",
    "    irradiance_col = [col for col in df.columns if 'irradiance' in col.lower()][0]\n",
    "    temp_col = [col for col in df.columns if 'temperature' in col.lower()][0]\n",
    "    \n",
    "    print(\"\\nüìä Estad√≠sticas comparativas:\")\n",
    "    \n",
    "    for nombre, anomalias in [(\"Euclidiana\", anomalias_euclidean), \n",
    "                              (\"Mahalanobis\", anomalias_mahalanobis), \n",
    "                              (\"Isolation Forest\", anomalias_isolation)]:\n",
    "        \n",
    "        if len(anomalias) > 0:\n",
    "            print(f\"\\n{nombre}:\")\n",
    "            \n",
    "            # Datos normales vs an√≥malos\n",
    "            datos_normales = df.drop(anomalias)\n",
    "            datos_anomalos = df.loc[anomalias]\n",
    "            \n",
    "            print(f\"  üåû Irradiancia promedio:\")\n",
    "            print(f\"     Normal: {datos_normales[irradiance_col].mean():.2f} W/m¬≤\")\n",
    "            print(f\"     An√≥malo: {datos_anomalos[irradiance_col].mean():.2f} W/m¬≤\")\n",
    "            \n",
    "            print(f\"  ‚ö° AC Power promedio:\")\n",
    "            print(f\"     Normal: {datos_normales['ac_power'].mean():.2f} W\")\n",
    "            print(f\"     An√≥malo: {datos_anomalos['ac_power'].mean():.2f} W\")\n",
    "            \n",
    "            print(f\"  üå°Ô∏è Temperatura promedio:\")\n",
    "            print(f\"     Normal: {datos_normales[temp_col].mean():.2f} ¬∞C\")\n",
    "            print(f\"     An√≥malo: {datos_anomalos[temp_col].mean():.2f} ¬∞C\")\n",
    "            \n",
    "            # An√°lisis temporal\n",
    "            anomalias_por_hora = datos_anomalos.index.hour.value_counts().sort_index()\n",
    "            hora_mas_anomalias = anomalias_por_hora.idxmax() if len(anomalias_por_hora) > 0 else \"N/A\"\n",
    "            print(f\"  üïê Hora con m√°s anomal√≠as: {hora_mas_anomalias}:00\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_lista_anomalias(anomalias_euclidean, anomalias_mahalanobis, anomalias_isolation):\n",
    "    \"\"\"\n",
    "    Generar lista con fechas de anomal√≠as detectadas\n",
    "    \"\"\"\n",
    "    print(\"\\nüìã RESUMEN DE ANOMAL√çAS DETECTADAS:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"\\n1Ô∏è‚É£ DISTANCIA EUCLIDIANA ({len(anomalias_euclidean)} anomal√≠as):\")\n",
    "    for fecha in sorted(anomalias_euclidean[:10]):  # Solo primeras 10 para no saturar\n",
    "        print(f\"   - {fecha.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    if len(anomalias_euclidean) > 10:\n",
    "        print(f\"   ... y {len(anomalias_euclidean) - 10} m√°s\")\n",
    "    print(f\"\\n2Ô∏è‚É£ DISTANCIA MAHALANOBIS ({len(anomalias_mahalanobis)} anomal√≠as):\")\n",
    "    for fecha in sorted(anomalias_mahalanobis[:10]):  # Solo primeras 10\n",
    "        print(f\"   - {fecha.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    if len(anomalias_mahalanobis) > 10:\n",
    "        print(f\"   ... y {len(anomalias_mahalanobis) - 10} m√°s\")\n",
    "    \n",
    "    print(f\"\\n3Ô∏è‚É£ ISOLATION FOREST ({len(anomalias_isolation)} anomal√≠as):\")\n",
    "    for fecha in sorted(anomalias_isolation[:10]):  # Solo primeras 10\n",
    "        print(f\"   - {fecha.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    if len(anomalias_isolation) > 10:\n",
    "        print(f\"   ... y {len(anomalias_isolation) - 10} m√°s\")\n",
    "    \n",
    "    # Encontrar anomal√≠as comunes\n",
    "    anomalias_comunes = set(anomalias_euclidean) & set(anomalias_mahalanobis) & set(anomalias_isolation)\n",
    "    print(f\"\\nüéØ ANOMAL√çAS DETECTADAS POR LOS 3 M√âTODOS ({len(anomalias_comunes)}):\")\n",
    "    for fecha in sorted(anomalias_comunes):\n",
    "        print(f\"   - {fecha.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Anomal√≠as √∫nicas de cada m√©todo\n",
    "    solo_euclidean = set(anomalias_euclidean) - set(anomalias_mahalanobis) - set(anomalias_isolation)\n",
    "    solo_mahalanobis = set(anomalias_mahalanobis) - set(anomalias_euclidean) - set(anomalias_isolation)\n",
    "    solo_isolation = set(anomalias_isolation) - set(anomalias_euclidean) - set(anomalias_mahalanobis)\n",
    "    \n",
    "    print(f\"\\nüî∏ Solo Euclidiana: {len(solo_euclidean)}\")\n",
    "    print(f\"üî∏ Solo Mahalanobis: {len(solo_mahalanobis)}\")\n",
    "    print(f\"üî∏ Solo Isolation Forest: {len(solo_isolation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 7. M√âTRICAS DE SEVERIDAD DE ANOMAL√çAS\n",
    "# ==========================================\n",
    "\n",
    "class SeveridadAnomalias:\n",
    "    \"\"\"\n",
    "    Sistema de m√©tricas para cuantificar la severidad de anomal√≠as\n",
    "    en paneles fotovoltaicos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.metricas_calculadas = False\n",
    "    \n",
    "    def calcular_metricas_severidad(self, df, anomalias_euclidean, anomalias_mahalanobis, \n",
    "                                  anomalias_isolation, dist_euclidean, dist_mahalanobis, \n",
    "                                  scores_isolation):\n",
    "        \"\"\"\n",
    "        Calcular m√∫ltiples m√©tricas de severidad para cada anomal√≠a\n",
    "        \"\"\"\n",
    "        print(\"üîç CALCULANDO M√âTRICAS DE SEVERIDAD\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Variables clave para paneles solares\n",
    "        irradiance_col = [col for col in df.columns if 'irradiance' in col.lower()][0]\n",
    "        temp_col = [col for col in df.columns if 'temperature' in col.lower()][0]\n",
    "        \n",
    "        # DataFrame para almacenar todas las m√©tricas\n",
    "        metricas_df = pd.DataFrame(index=df.index)\n",
    "        \n",
    "        # ========================================\n",
    "        # 1. M√âTRICA DE DISTANCIA NORMALIZADA\n",
    "        # ========================================\n",
    "        \n",
    "        # Normalizar distancias euclidianas (0-100)\n",
    "        dist_euclidean_norm = pd.Series(dist_euclidean, index=df.index)\n",
    "        metricas_df['distancia_euclidiana'] = (dist_euclidean_norm / dist_euclidean_norm.max()) * 100\n",
    "        \n",
    "        # Normalizar distancias mahalanobis (0-100)\n",
    "        dist_mahalanobis_norm = pd.Series(dist_mahalanobis, index=df.index)\n",
    "        metricas_df['distancia_mahalanobis'] = (dist_mahalanobis_norm / dist_mahalanobis_norm.max()) * 100\n",
    "        \n",
    "        # Normalizar scores isolation forest (-1 a 1 ‚Üí 0 a 100)\n",
    "        scores_isolation_norm = pd.Series(scores_isolation, index=df.index)\n",
    "        metricas_df['score_isolation'] = ((scores_isolation_norm - scores_isolation_norm.min()) / \n",
    "                                        (scores_isolation_norm.max() - scores_isolation_norm.min())) * 100\n",
    "        \n",
    "        # ========================================\n",
    "        # 2. M√âTRICA DE EFICIENCIA AN√ìMALA\n",
    "        # ========================================\n",
    "        \n",
    "        # Eficiencia esperada vs real\n",
    "        eficiencia_esperada = self._calcular_eficiencia_esperada(df, irradiance_col, temp_col)\n",
    "        eficiencia_real = df['ac_power'] / (df[irradiance_col] + 1e-6)  # Evitar divisi√≥n por 0\n",
    "        \n",
    "        # P√©rdida de eficiencia (0-100%, donde 100% = p√©rdida total)\n",
    "        perdida_eficiencia = np.maximum(0, (eficiencia_esperada - eficiencia_real) / eficiencia_esperada * 100)\n",
    "        metricas_df['perdida_eficiencia'] = np.clip(perdida_eficiencia, 0, 100)\n",
    "        \n",
    "        # ========================================\n",
    "        # 3. M√âTRICA DE DESVIACI√ìN Z-SCORE\n",
    "        # ========================================\n",
    "        \n",
    "        # Z-score para variables clave\n",
    "        metricas_df['zscore_ac_power'] = np.abs(stats.zscore(df['ac_power']))\n",
    "        metricas_df['zscore_ac_voltage'] = np.abs(stats.zscore(df['ac_voltage']))\n",
    "        metricas_df['zscore_eficiencia'] = np.abs(stats.zscore(eficiencia_real))\n",
    "        \n",
    "        # ========================================\n",
    "        # 4. M√âTRICA DE CORRELACI√ìN ROTA\n",
    "        # ========================================\n",
    "        \n",
    "        # Correlaci√≥n local vs global (ventana m√≥vil)\n",
    "        correlacion_global = df[irradiance_col].corr(df['ac_power'])\n",
    "        correlacion_local = df[irradiance_col].rolling(window=24).corr(df['ac_power']).fillna(correlacion_global)\n",
    "        \n",
    "        # Qu√© tanto se desv√≠a de la correlaci√≥n esperada\n",
    "        metricas_df['correlacion_rota'] = np.abs(correlacion_global - correlacion_local) * 100\n",
    "        \n",
    "        # ========================================\n",
    "        # 5. SCORE COMPUESTO DE SEVERIDAD\n",
    "        # ========================================\n",
    "        \n",
    "        # Combinar m√∫ltiples m√©tricas con pesos\n",
    "        pesos = {\n",
    "            'distancia_mahalanobis': 0.25,    # M√°s peso por sensibilidad a correlaciones\n",
    "            'perdida_eficiencia': 0.30,       # M√©trica espec√≠fica de paneles solares\n",
    "            'distancia_euclidiana': 0.20,     # Distancia b√°sica\n",
    "            'score_isolation': 0.15,          # Detecci√≥n ML\n",
    "            'correlacion_rota': 0.10          # Correlaciones an√≥malas\n",
    "        }\n",
    "        \n",
    "        score_severidad = (\n",
    "            metricas_df['distancia_mahalanobis'] * pesos['distancia_mahalanobis'] +\n",
    "            metricas_df['perdida_eficiencia'] * pesos['perdida_eficiencia'] +\n",
    "            metricas_df['distancia_euclidiana'] * pesos['distancia_euclidiana'] +\n",
    "            metricas_df['score_isolation'] * pesos['score_isolation'] +\n",
    "            metricas_df['correlacion_rota'] * pesos['correlacion_rota']\n",
    "        )\n",
    "        \n",
    "        metricas_df['severidad_compuesta'] = score_severidad\n",
    "        \n",
    "        # ========================================\n",
    "        # 6. CLASIFICACI√ìN DE SEVERIDAD\n",
    "        # ========================================\n",
    "        \n",
    "        def clasificar_severidad(score):\n",
    "            if score >= 80:\n",
    "                return \"üî¥ CR√çTICA\"\n",
    "            elif score >= 60:\n",
    "                return \"üü† ALTA\"\n",
    "            elif score >= 40:\n",
    "                return \"üü° MEDIA\"\n",
    "            elif score >= 20:\n",
    "                return \"üü¢ BAJA\"\n",
    "            else:\n",
    "                return \"‚ö™ NORMAL\"\n",
    "        \n",
    "        metricas_df['clasificacion'] = metricas_df['severidad_compuesta'].apply(clasificar_severidad)\n",
    "        \n",
    "        self.metricas_df = metricas_df\n",
    "        self.metricas_calculadas = True\n",
    "        \n",
    "        return metricas_df\n",
    "    \n",
    "    def _calcular_eficiencia_esperada(self, df, irradiance_col, temp_col):\n",
    "        \"\"\"\n",
    "        Calcular eficiencia esperada basada en condiciones ambientales\n",
    "        Modelo simplificado: Eficiencia = f(irradiancia, temperatura)\n",
    "        \"\"\"\n",
    "        # Eficiencia base normalizada por irradiancia\n",
    "        eficiencia_base = df[irradiance_col] * 0.02  # Factor t√≠pico de conversi√≥n\n",
    "        \n",
    "        # Correcci√≥n por temperatura (los paneles son menos eficientes con calor)\n",
    "        temperatura_optima = 25  # ¬∞C\n",
    "        factor_temperatura = 1 - (np.maximum(0, df[temp_col] - temperatura_optima) * 0.004)\n",
    "        \n",
    "        return eficiencia_base * factor_temperatura\n",
    "    \n",
    "    def analizar_anomalias_por_severidad(self, anomalias_euclidean, anomalias_mahalanobis, \n",
    "                                       anomalias_isolation):\n",
    "        \"\"\"\n",
    "        Analizar las anomal√≠as detectadas por nivel de severidad\n",
    "        \"\"\"\n",
    "        if not self.metricas_calculadas:\n",
    "            raise ValueError(\"Primero calcule las m√©tricas con calcular_metricas_severidad()\")\n",
    "        \n",
    "        print(\"\\nüéØ AN√ÅLISIS POR SEVERIDAD\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        # Todas las anomal√≠as √∫nicas\n",
    "        todas_anomalias = set(anomalias_euclidean) | set(anomalias_mahalanobis) | set(anomalias_isolation)\n",
    "        \n",
    "        # Crear reporte de severidad\n",
    "        reporte_severidad = []\n",
    "        \n",
    "        for fecha in todas_anomalias:\n",
    "            if fecha in self.metricas_df.index:\n",
    "                fila = self.metricas_df.loc[fecha]\n",
    "                \n",
    "                # Determinar qu√© m√©todos la detectaron\n",
    "                detectores = []\n",
    "                if fecha in anomalias_euclidean:\n",
    "                    detectores.append(\"Euclidiana\")\n",
    "                if fecha in anomalias_mahalanobis:\n",
    "                    detectores.append(\"Mahalanobis\")\n",
    "                if fecha in anomalias_isolation:\n",
    "                    detectores.append(\"Isolation\")\n",
    "                \n",
    "                reporte_severidad.append({\n",
    "                    'fecha': fecha,\n",
    "                    'severidad_score': fila['severidad_compuesta'],\n",
    "                    'clasificacion': fila['clasificacion'],\n",
    "                    'perdida_eficiencia': fila['perdida_eficiencia'],\n",
    "                    'dist_mahalanobis': fila['distancia_mahalanobis'],\n",
    "                    'detectores': ', '.join(detectores),\n",
    "                    'num_detectores': len(detectores)\n",
    "                })\n",
    "        \n",
    "        # Convertir a DataFrame y ordenar por severidad\n",
    "        reporte_df = pd.DataFrame(reporte_severidad)\n",
    "        reporte_df = reporte_df.sort_values('severidad_score', ascending=False)\n",
    "        \n",
    "        # Mostrar top 10 m√°s severas\n",
    "        print(\"\\nüî• TOP 10 ANOMAL√çAS M√ÅS SEVERAS:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Fecha':<20} {'Severidad':<12} {'Clase':<15} {'P√©rdida %':<10} {'Detectores'}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for _, row in reporte_df.head(10).iterrows():\n",
    "            fecha_str = row['fecha'].strftime('%Y-%m-%d %H:%M')\n",
    "            print(f\"{fecha_str:<20} {row['severidad_score']:<11.1f} {row['clasificacion']:<15} \"\n",
    "                  f\"{row['perdida_eficiencia']:<9.1f}% {row['detectores']}\")\n",
    "        \n",
    "        # Estad√≠sticas por clasificaci√≥n\n",
    "        print(f\"\\nüìä DISTRIBUCI√ìN POR SEVERIDAD:\")\n",
    "        clasificaciones = reporte_df['clasificacion'].value_counts()\n",
    "        for clase, count in clasificaciones.items():\n",
    "            print(f\"   {clase}: {count} anomal√≠as\")\n",
    "        \n",
    "        return reporte_df\n",
    "    \n",
    "    def plot_severidad_timeline(self, df, reporte_df):\n",
    "        \"\"\"\n",
    "        Graficar timeline de severidad de anomal√≠as\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
    "        \n",
    "        # Gr√°fica 1: AC Power con anomal√≠as coloreadas por severidad\n",
    "        ax1.plot(df.index, df['ac_power'], alpha=0.6, color='lightblue', label='AC Power Normal')\n",
    "        \n",
    "        # Colorear anomal√≠as por severidad\n",
    "        colores_severidad = {\n",
    "            'üî¥ CR√çTICA': 'red',\n",
    "            'üü† ALTA': 'orange', \n",
    "            'üü° MEDIA': 'yellow',\n",
    "            'üü¢ BAJA': 'green'\n",
    "        }\n",
    "        \n",
    "        for clase, color in colores_severidad.items():\n",
    "            anomalias_clase = reporte_df[reporte_df['clasificacion'] == clase]\n",
    "            if len(anomalias_clase) > 0:\n",
    "                fechas = anomalias_clase['fecha']\n",
    "                valores = df.loc[fechas, 'ac_power'] if len(fechas) > 0 else []\n",
    "                ax1.scatter(fechas, valores, c=color, s=50, alpha=0.8, label=clase)\n",
    "        \n",
    "        ax1.set_ylabel('AC Power (W)')\n",
    "        ax1.set_title('Timeline de Anomal√≠as por Severidad')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "        \n",
    "        # Gr√°fica 2: Distribuci√≥n de scores de severidad\n",
    "        ax2.hist(reporte_df['severidad_score'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax2.axvline(reporte_df['severidad_score'].mean(), color='red', linestyle='--', \n",
    "                   label=f'Media: {reporte_df[\"severidad_score\"].mean():.1f}')\n",
    "        ax2.set_xlabel('Score de Severidad')\n",
    "        ax2.set_ylabel('Frecuencia')\n",
    "        ax2.set_title('Distribuci√≥n de Scores de Severidad')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def aplicar_metricas_severidad(df_hourly, anomalias_euclidean, anomalias_mahalanobis, \n",
    "                              anomalias_isolation, dist_euclidean, dist_mahalanobis, \n",
    "                              scores_isolation):\n",
    "    \"\"\"\n",
    "    Funci√≥n principal para aplicar todas las m√©tricas de severidad\n",
    "    \"\"\"\n",
    "    # Crear instancia del analizador\n",
    "    analizador = SeveridadAnomalias()\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    metricas_df = analizador.calcular_metricas_severidad(\n",
    "        df_hourly, anomalias_euclidean, anomalias_mahalanobis, \n",
    "        anomalias_isolation, dist_euclidean, dist_mahalanobis, scores_isolation\n",
    "    )\n",
    "    \n",
    "    # Analizar por severidad\n",
    "    reporte_severidad = analizador.analizar_anomalias_por_severidad(\n",
    "        anomalias_euclidean, anomalias_mahalanobis, anomalias_isolation\n",
    "    )\n",
    "    \n",
    "    # Graficar timeline\n",
    "    analizador.plot_severidad_timeline(df_hourly, reporte_severidad)\n",
    "    \n",
    "    return metricas_df, reporte_severidad, analizador\n",
    "\n",
    "# ==========================================\n",
    "# 8. FUNCI√ìN PRINCIPAL MEJORADA\n",
    "# ==========================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci√≥n principal que ejecuta todo el an√°lisis con visualizaciones mejoradas\n",
    "    \"\"\"\n",
    "    print(\"üîã AN√ÅLISIS DE ANOMAL√çAS EN PANELES FOTOVOLTAICOS\")\n",
    "    print(\"=\"*65)\n",
    "    \n",
    "    # 1. Cargar datasets\n",
    "    environment_data, irradiance_data, electrical_data = cargar_datasets()\n",
    "    \n",
    "    if any(data is None for data in [environment_data, irradiance_data, electrical_data]):\n",
    "        print(\"‚ùå Error: No se pudieron cargar todos los datasets necesarios\")\n",
    "        return\n",
    "    \n",
    "    # 2. Combinar datasets\n",
    "    df_combined = combinar_datasets(environment_data, irradiance_data, electrical_data)\n",
    "    \n",
    "    # 3. Crear promedios horarios\n",
    "    df_hourly = crear_promedios_horarios(df_combined, \n",
    "        metodo_agregacion='mean',  # Puedes cambiar a 'median' o 'weighted_mean'\n",
    "        incluir_estadisticas=True)\n",
    "\n",
    "    df_hourly.info()\n",
    "    \n",
    "    print(f\"\\nüìä Dataset final para an√°lisis:\")\n",
    "    print(f\"   - Filas: {len(df_hourly)}\")\n",
    "    print(f\"   - Columnas: {len(df_hourly.columns)}\")\n",
    "    print(f\"   - Rango de fechas: {df_hourly.index.min()} a {df_hourly.index.max()}\")\n",
    "    \n",
    "    # 4. Detectar anomal√≠as con los 3 m√©todos (par√°metros m√°s selectivos)\n",
    "    anomalias_euclidean, dist_euclidean, umbral_euclidean = detector_euclidean_distance(df_hourly, percentil=99)\n",
    "    anomalias_mahalanobis, dist_mahalanobis, umbral_mahalanobis = detector_mahalanobis_distance(df_hourly, percentil=99)\n",
    "    anomalias_isolation, scores_isolation = detector_isolation_forest(df_hourly, contaminacion=0.01)\n",
    "    \n",
    "    # 5. Analizar calidad de las anomal√≠as\n",
    "    analizar_calidad_anomalias(df_hourly, anomalias_euclidean, anomalias_mahalanobis, anomalias_isolation)\n",
    "    \n",
    "    # ========================================\n",
    "    # 6. VISUALIZACIONES MEJORADAS\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\\nüé® GENERANDO VISUALIZACIONES MEJORADAS...\")\n",
    "    \n",
    "    # Gr√°fica espec√≠fica de distancia euclidiana con m√°scara\n",
    "    plot_euclidean_distance_with_mask(df_hourly, anomalias_euclidean, dist_euclidean, \n",
    "                                     umbral_euclidean, variable_principal='ac_power')\n",
    "    \n",
    "    # Gr√°fica comparativa mejorada (incluye distancias euclidianas)\n",
    "    plot_anomalies_comparison_mejorado(df_hourly, anomalias_euclidean, anomalias_mahalanobis, \n",
    "                                      anomalias_isolation, dist_euclidean, dist_mahalanobis, \n",
    "                                      scores_isolation, umbral_euclidean, umbral_mahalanobis,\n",
    "                                      variable_principal='ac_power')\n",
    "    \n",
    "    # 7. NUEVO: Calcular m√©tricas de severidad\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ AN√ÅLISIS DE SEVERIDAD DE ANOMAL√çAS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    metricas_df, reporte_severidad, analizador = aplicar_metricas_severidad(\n",
    "        df_hourly, \n",
    "        anomalias_euclidean, \n",
    "        anomalias_mahalanobis, \n",
    "        anomalias_isolation,\n",
    "        dist_euclidean,\n",
    "        dist_mahalanobis, \n",
    "        scores_isolation\n",
    "    )\n",
    "    \n",
    "    # 8. Generar lista de anomal√≠as\n",
    "    generar_lista_anomalias(anomalias_euclidean, anomalias_mahalanobis, anomalias_isolation)\n",
    "    \n",
    "    return df_combined,df_hourly, anomalias_euclidean, anomalias_mahalanobis, anomalias_isolation, metricas_df, reporte_severidad\n",
    "\n",
    "# ==========================================\n",
    "# 9. FUNCI√ìN ALTERNATIVA PARA USAR SOLO EUCLIDIANA\n",
    "# ==========================================\n",
    "\n",
    "def analisis_solo_euclidiana():\n",
    "    \"\"\"\n",
    "    Funci√≥n enfocada solo en el an√°lisis de distancia euclidiana con m√°scara\n",
    "    \"\"\"\n",
    "    print(\"üîã AN√ÅLISIS EUCLIDIANO DE ANOMAL√çAS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Cargar y preparar datos\n",
    "    environment_data, irradiance_data, electrical_data = cargar_datasets()\n",
    "    \n",
    "    if any(data is None for data in [environment_data, irradiance_data, electrical_data]):\n",
    "        print(\"‚ùå Error: No se pudieron cargar todos los datasets necesarios\")\n",
    "        return\n",
    "    \n",
    "    df_combined = combinar_datasets(environment_data, irradiance_data, electrical_data)\n",
    "    df_hourly = crear_promedios_horarios(df_combined)\n",
    "    \n",
    "    # Solo detectar anomal√≠as euclidianas\n",
    "    anomalias_euclidean, dist_euclidean, umbral_euclidean = detector_euclidean_distance(df_hourly, percentil=99)\n",
    "    \n",
    "    # Visualizaci√≥n espec√≠fica\n",
    "    plot_euclidean_distance_with_mask(df_hourly, anomalias_euclidean, dist_euclidean, \n",
    "                                     umbral_euclidean, variable_principal='ac_power')\n",
    "    \n",
    "    return df_hourly, anomalias_euclidean, dist_euclidean, umbral_euclidean\n",
    "\n",
    "# ==========================================\n",
    "# 10. EJECUCI√ìN\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ejecutar an√°lisis completo\n",
    "    resultado = main()\n",
    "    \n",
    "    # O ejecutar solo an√°lisis euclidiano:\n",
    "    # resultado_euclidiano = analisis_solo_euclidiana()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 1. Prepara tu DataFrame\n",
    "# 1. Prepara tu DataFrame\n",
    "df = resultado[0].copy()\n",
    "df = df[['measured_on', 'ac_power']].dropna()\n",
    "df = df[df['ac_power'] > 0]  # Solo valores positivos\n",
    "\n",
    "# Filtro IQR m√°s estricto\n",
    "Q1 = df['ac_power'].quantile(0.25)\n",
    "Q3 = df['ac_power'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "df = df[(df['ac_power'] >= Q1 - 1.0 * IQR) & (df['ac_power'] <= Q3 + 1.0 * IQR)]\n",
    "\n",
    "# Filtro por percentiles extremos\n",
    "lower = df['ac_power'].quantile(0.02)\n",
    "upper = df['ac_power'].quantile(0.98)\n",
    "df = df[(df['ac_power'] >= lower) & (df['ac_power'] <= upper)]\n",
    "\n",
    "# Suavizado m√°s fuerte\n",
    "\n",
    "\n",
    "# 2. Resampleo horario\n",
    "df_hourly = df.set_index('measured_on').resample('D').mean().dropna().reset_index()\n",
    "\n",
    "# ‚úÖ Detecci√≥n de gaps grandes\n",
    "df_hourly['gap'] = df_hourly['measured_on'].diff() > pd.Timedelta('1.5H')\n",
    "print(f\"{df_hourly['gap'].sum()} gaps mayores a 1.5 horas\")\n",
    "\n",
    "# 3. Prepara datos para Prophet\n",
    "df_prophet = df_hourly.rename(columns={'measured_on': 'ds', 'ac_power': 'y'})[['ds', 'y']]\n",
    "print(df_prophet.head())\n",
    "# ‚úÖ Filtra valores peque√±os para evitar distorsiones en MAPE\n",
    "df_prophet = df_prophet[df_prophet['y'] > 5]\n",
    "\n",
    "# 4. Entrenamiento\n",
    "print(\"üîÆ INICIANDO AN√ÅLISIS CON PROPHET\")\n",
    "print(\"=\" * 40)\n",
    "model = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False,\n",
    "    seasonality_mode='additive'\n",
    ")\n",
    "model.fit(df_prophet)\n",
    "\n",
    "# 5. Predicci√≥n a futuro\n",
    "# Encuentra la √∫ltima fecha de tu serie\n",
    "last_date = df_prophet['ds'].max()\n",
    "# Define la fecha final deseada\n",
    "end_date = pd.Timestamp('2025-12-31 23:00:00')\n",
    "# Calcula cu√°ntas horas hay entre ambas fechas\n",
    "periods = int((end_date - last_date) / pd.Timedelta(hours=1))\n",
    "\n",
    "# Genera el dataframe futuro hasta 2025\n",
    "future = model.make_future_dataframe(periods=360)\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# ‚úÖ Evitar valores negativos\n",
    "forecast['yhat'] = forecast['yhat'].clip(lower=0)\n",
    "forecast['yhat_lower'] = forecast['yhat_lower'].clip(lower=0)\n",
    "forecast['yhat_upper'] = forecast['yhat_upper'].clip(lower=0)\n",
    "\n",
    "# 6. Visualizaci√≥n \n",
    "fig1 = model.plot(forecast)\n",
    "plt.title('üîã Predicci√≥n de Producci√≥n de Energ√≠a (ac_power)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('ac_power')\n",
    "plt.show()\n",
    "\n",
    "fig2 = model.plot_components(forecast)\n",
    "plt.show()\n",
    "\n",
    "# 7. Evaluaci√≥n del modelo\n",
    "y_true = df_prophet['y'].reset_index(drop=True)\n",
    "y_pred = forecast['yhat'][:len(y_true)].reset_index(drop=True)\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "mape = np.mean(np.abs((y_true - y_pred) / y_true.replace(0, np.nan))) * 100\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "smape_val = smape(y_true, y_pred)\n",
    "\n",
    "# 8. Mostrar m√©tricas\n",
    "print(f\"üìè MAE:  {mae:.2f}\")\n",
    "print(f\"üìâ RMSE: {rmse:.2f}\")\n",
    "print(f\"üìä MAPE: {mape:.2f}%\")\n",
    "print(f\"üìä SMAPE: {smape_val:.2f}%\")\n",
    "\n",
    "# 9. An√°lisis de residuos\n",
    "residuals = y_true - y_pred\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('üìà An√°lisis de Residuos')\n",
    "plt.xlabel('Predicci√≥n')\n",
    "plt.ylabel('Error (residuo)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
